{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/lzw365/Downloads/DeepEverest/test.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000000vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, transforms\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000000vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m \u001b[39mimport\u001b[39;00m Variable\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000000vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnx\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000000vscode-remote?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnx_tf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m prepare\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnx'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n",
    "                    epoch,  loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the maxlog-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])), batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, \n",
    "                        transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])), batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train Epoch: 0 \tLoss: 2.324259\n",
      "\n",
      "Test set: Average loss: 0.1785, Accuracy: 9446/10000 (94%)\n",
      "\n",
      "Train Epoch: 1 \tLoss: 0.256421\n",
      "\n",
      "Test set: Average loss: 0.1137, Accuracy: 9660/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 \tLoss: 0.271181\n",
      "\n",
      "Test set: Average loss: 0.0918, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 \tLoss: 0.208402\n",
      "\n",
      "Test set: Average loss: 0.0773, Accuracy: 9751/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 \tLoss: 0.334902\n",
      "\n",
      "Test set: Average loss: 0.0697, Accuracy: 9781/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 \tLoss: 0.089708\n",
      "\n",
      "Test set: Average loss: 0.0661, Accuracy: 9770/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 \tLoss: 0.141611\n",
      "\n",
      "Test set: Average loss: 0.0611, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 \tLoss: 0.170826\n",
      "\n",
      "Test set: Average loss: 0.0568, Accuracy: 9823/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 \tLoss: 0.139686\n",
      "\n",
      "Test set: Average loss: 0.0549, Accuracy: 9819/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 \tLoss: 0.079753\n",
      "\n",
      "Test set: Average loss: 0.0561, Accuracy: 9833/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "model_torch = Net().to(device)\n",
    "optimizer = optim.SGD(model_torch.parameters(), lr=0.01, momentum=0.5)\n",
    "for epoch in range(10):\n",
    "    train(model_torch, device, train_loader, optimizer, epoch)\n",
    "    test(model_torch, device, test_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_torch.state_dict(), 'mnist.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_torch = Net()\n",
    "model_torch.load_state_dict(torch.load('mnist.pth'))\n",
    "dummy_input = Variable(torch.randn(1, 1, 28, 28))\n",
    "torch.onnx.export(model_torch.eval(), dummy_input, \"mnist.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onnx = onnx.load('mnist.onnx')\n",
    "model_tf = prepare(model_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAAnRSTlMAAHaTzTgAAADnSURBVHic1ZAxawJBEIUfEizlsFNsDntlalMExGZ/gj/h2sX8CYvrLdNvIwipr7h/kGIJpPREtxk7Q8JLlcBtbrs0mWqYjzfz3gB/VlPvk6xseEigUU3Wo24mjrTLhPCe1HXKy1lpUmYe1SXuARV93p70fjozx/7WhnffTbYaXE9HAHbysYmXZhUpgLUXkvFpQ/oZxJHqWY3bwoLnLfKncHGyoB9HQrXIHLXsz150F2UkgZrvBkvlFjF0kFda5D4UcZThFfjsY+6Hbw9NnERUpQgkm47vSlAlyVD+ZoB5ZuCukC72P+oLx0RrywwWOBcAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x27C6967C250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n",
      "The digit is classified as [[-1172.1316 -1628.2738 -1390.8823 -2273.5647 -1083.5457 -1100.4241\n",
      "      0.     -2661.3796 -1179.9258 -1963.0388]] pytorch\n",
      "The digit is classified as Outputs(_0=array([[-1172.1315, -1628.2738, -1390.8823, -2273.5647, -1083.5457,\n",
      "        -1100.424 ,     0.    , -2661.3796, -1179.9258, -1963.0388]],\n",
      "      dtype=float32)) tf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Image 1:')\n",
    "img = Image.open('two.png').resize((28, 28)).convert('L')\n",
    "display(img)\n",
    "temp = np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :]\n",
    "print(temp.shape)\n",
    "output = model_torch(torch.from_numpy(temp))\n",
    "print('The digit is classified as', output.detach().numpy(), 'pytorch')\n",
    "output = model_tf.run(temp)\n",
    "print('The digit is classified as', output, 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 2:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABf0lEQVR4nGNgoD9gxBBg5GX4+QNDkpmBQ5BJXIjLg+HattsMDAwMDCxwHULyzHpRrNJSTKz/f+qWfEAxTnbjoyfvfv/5/fPLu89/XzexIBvLaLuZ7/+/rx8fPLhzU7OY54bRdyRj/z86p/3ux649l9/8+6eZysOKaufjYNZ//3/8/M3AIBDKx/DjP1a/sUhVvfj/towZRScDA4uOyOc3HAZR9lzv1878i+Z7iUXfPh+5+v3P/1fTlNHNY3d/9v//n7///j6plGXGCDfJaQeePH3371M+B9wiuOT/5/nc+mwuhf9//UKWZJF8/YOBgYHh94ejrEr/WaWZ/0ElmRgYWK2W+UN5f9itWX6eRnapzNbPM/kgwchi9/z/PWFkl6jc+vvpiBcvCyu3Zv75v5dDWZAlOX3O/vr/cmFiyuyLX//f8kXzBpP6mi///3z9+vvft6uZrBhetF726ufPn9+fzrNFkYM4hFVE3Zv137NDN7/+wZRkYGBkZ2D494uBWgAANqOZcFTmPBAAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x27CBDB05C40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n",
      "The digit is classified as [[-1828.054  -1347.0144 -1056.4312     0.     -1870.2683 -1012.4519\n",
      "  -2525.1013 -1003.2545 -1007.5098  -741.2179]] pytorch\n",
      "The digit is classified as Outputs(_0=array([[-1828.0537 , -1347.0143 , -1056.4309 ,     0.     , -1870.2682 ,\n",
      "        -1012.452  , -2525.101  , -1003.25433, -1007.50964,  -741.2178 ]],\n",
      "      dtype=float32)) tf\n"
     ]
    }
   ],
   "source": [
    "print('Image 2:')\n",
    "img = Image.open('three.png').resize((28, 28)).convert('L')\n",
    "display(img)\n",
    "temp = np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :]\n",
    "print(temp.shape)\n",
    "output = model_torch(torch.from_numpy(temp))\n",
    "print('The digit is classified as', output.detach().numpy(), 'pytorch')\n",
    "output = model_tf.run(temp)\n",
    "print('The digit is classified as', output, 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `__call__` contains input name(s) input.1 with unsupported characters which will be renamed to input_1 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist.pb\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist.pb\\assets\n"
     ]
    }
   ],
   "source": [
    "model_tf.export_graph('mnist.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 2.3621e-01,  1.3497e-01, -1.2790e-01, -1.0865e-01,  9.7583e-02],\n",
      "          [ 2.9348e-01,  8.4625e-02, -3.5057e-03, -2.9974e-01,  1.1580e-01],\n",
      "          [-8.7698e-03,  1.2850e-02, -1.6791e-01, -1.4662e-01, -1.2763e-01],\n",
      "          [-4.4721e-02, -1.6092e-01,  1.3615e-01,  2.2192e-01,  2.4346e-02],\n",
      "          [ 1.4238e-01,  1.7062e-01,  2.7300e-01, -1.3579e-01, -1.1499e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4378e-01,  3.6939e-01,  7.3117e-02,  1.8251e-01, -1.6565e-01],\n",
      "          [ 9.3901e-03, -5.7588e-02, -3.1732e-01, -2.9014e-01, -3.0704e-01],\n",
      "          [-2.3215e-01, -4.2892e-01, -3.7199e-01, -2.3679e-01,  5.1020e-02],\n",
      "          [-2.4692e-01, -2.4577e-01,  4.8212e-02,  3.3047e-02,  3.1894e-01],\n",
      "          [ 2.0490e-01,  1.9171e-01,  3.3009e-01,  2.9320e-01,  2.3797e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5635e-01,  2.2497e-01,  1.0351e-01, -1.3179e-01, -1.0490e-01],\n",
      "          [-1.1305e-01,  2.5210e-01,  2.5358e-01,  5.9893e-02, -2.4075e-01],\n",
      "          [ 3.3765e-02,  1.6204e-01,  5.0856e-02,  5.0694e-02, -2.6962e-01],\n",
      "          [ 9.1991e-03,  1.6666e-01,  1.6610e-01,  2.4418e-01,  1.4504e-01],\n",
      "          [ 4.5848e-02,  1.3321e-01,  1.7494e-01,  1.4552e-01,  2.6637e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3651e-01,  1.5970e-01,  5.2587e-02, -1.0143e-01,  2.4040e-01],\n",
      "          [ 3.0846e-01,  2.7383e-01, -1.1526e-01, -2.3024e-03,  2.6199e-01],\n",
      "          [ 1.8533e-01,  6.7001e-02, -3.0092e-01, -2.5718e-01, -1.0010e-01],\n",
      "          [ 1.4842e-02, -8.6288e-02, -7.0327e-02, -1.8635e-01,  2.2182e-01],\n",
      "          [ 1.1968e-01,  1.8220e-01,  1.7814e-01,  9.6312e-02, -9.8057e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.8025e-02, -1.1776e-01,  1.7925e-01,  2.4380e-01,  2.2062e-01],\n",
      "          [ 2.6215e-02,  3.4500e-02,  3.4182e-01,  8.3529e-02, -2.5112e-02],\n",
      "          [ 1.9655e-01,  2.8367e-01,  2.5912e-01,  2.7613e-04, -1.7107e-01],\n",
      "          [ 2.9546e-01,  1.6122e-01, -1.7232e-01, -1.3055e-01, -2.7592e-01],\n",
      "          [ 1.7737e-01, -1.4660e-01, -1.0000e-01, -2.4660e-01, -2.5675e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.3910e-02, -2.5688e-01, -1.3353e-01, -2.3716e-01,  9.1301e-02],\n",
      "          [-8.2195e-02, -3.2301e-01, -3.1055e-01, -3.1086e-01, -1.1623e-01],\n",
      "          [ 3.3999e-01,  2.8031e-02, -2.6158e-01, -2.6995e-01, -2.9325e-01],\n",
      "          [ 1.9333e-01,  3.5061e-01,  1.9955e-01,  3.0524e-01,  2.7291e-01],\n",
      "          [-2.1546e-01,  2.5303e-01,  4.6465e-01,  3.9647e-01,  2.1803e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.5712e-03,  2.1572e-01,  1.2296e-01, -1.6118e-01, -1.2183e-01],\n",
      "          [ 1.9101e-02,  2.1611e-01,  2.0268e-01, -3.8072e-02, -3.7569e-01],\n",
      "          [ 5.3621e-02,  3.8574e-01,  1.0444e-01, -1.5111e-01, -4.3433e-01],\n",
      "          [ 2.5773e-01,  2.7998e-01,  3.2458e-02, -3.2163e-01, -2.9865e-01],\n",
      "          [ 5.9338e-02,  2.4409e-01,  7.1784e-02, -3.2898e-02, -2.6891e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8324e-01,  4.3488e-02,  3.5552e-01,  1.2053e-01,  2.3707e-01],\n",
      "          [ 2.7933e-01,  1.7742e-01,  3.1680e-02,  3.8413e-01,  4.6380e-01],\n",
      "          [-2.4446e-01,  1.7628e-01,  1.8548e-01,  9.1320e-02,  2.7336e-01],\n",
      "          [-4.7194e-01, -3.5852e-01, -3.3194e-01, -3.6462e-01, -9.3975e-02],\n",
      "          [-1.7929e-01, -4.3340e-01, -3.7215e-01, -3.5485e-01, -2.3065e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.0257e-02, -3.0922e-01, -1.3636e-01, -1.4642e-01,  2.1731e-01],\n",
      "          [-2.5778e-01, -2.2895e-01, -2.3114e-01,  9.1980e-02,  2.4476e-01],\n",
      "          [-3.5534e-01, -3.1055e-01, -8.8293e-02,  2.2370e-01,  2.9219e-01],\n",
      "          [-2.9598e-01, -1.6241e-01, -9.0287e-02, -4.0925e-03,  4.6515e-01],\n",
      "          [-3.1073e-01, -1.6013e-03, -4.6260e-02,  1.6863e-01,  3.9876e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1946e-01,  1.6365e-02, -2.2907e-01, -1.1312e-01,  3.3189e-03],\n",
      "          [ 2.1584e-01,  7.8740e-02,  8.3809e-02, -7.9832e-02, -2.5716e-01],\n",
      "          [ 2.1488e-01,  3.0340e-01,  7.3759e-02,  2.4300e-01, -1.0309e-01],\n",
      "          [-1.2532e-01,  7.0450e-02,  1.9295e-01,  3.5601e-01,  2.3866e-01],\n",
      "          [-3.6162e-01, -2.0417e-01, -8.5188e-02, -2.3655e-01,  1.1571e-01]]]])\n",
      "torch.Size([10, 1, 5, 5])\n",
      "tensor([-0.1328, -0.0792, -0.1443,  0.1314, -0.0352,  0.0069, -0.1948,  0.0976,\n",
      "        -0.0614,  0.0684])\n",
      "torch.Size([10])\n",
      "tensor([[[[ 4.0823e-02,  6.6269e-03, -6.3182e-02,  1.7227e-02,  8.3316e-02],\n",
      "          [-2.0102e-02, -8.4355e-02, -1.0102e-02, -6.5888e-02,  3.6888e-03],\n",
      "          [ 6.3970e-02, -4.1646e-02, -7.8800e-02, -4.2350e-02,  8.7714e-02],\n",
      "          [ 9.0821e-02,  3.8878e-02, -6.7623e-02,  5.7687e-02,  8.4510e-02],\n",
      "          [ 6.7407e-02,  3.7067e-02,  9.5678e-03,  4.7655e-02, -4.7372e-02]],\n",
      "\n",
      "         [[-1.4257e-01, -1.3895e-01, -8.3205e-02, -9.9845e-02, -1.0901e-01],\n",
      "          [-9.5861e-02, -4.6636e-02, -7.2163e-02, -3.6806e-02, -5.5683e-02],\n",
      "          [ 1.4985e-02, -8.4171e-04,  4.6845e-02, -4.3587e-02, -1.1249e-01],\n",
      "          [ 3.9592e-02, -4.2661e-03, -3.4522e-02, -3.5308e-02, -4.9223e-02],\n",
      "          [ 5.8993e-02, -7.4555e-02,  2.6807e-02, -3.0622e-02, -2.8920e-02]],\n",
      "\n",
      "         [[ 4.5332e-02, -9.2012e-02,  6.2907e-02,  7.7445e-02,  8.3904e-02],\n",
      "          [ 4.7060e-02, -7.1568e-02, -8.7103e-02, -2.7472e-02,  1.3445e-01],\n",
      "          [ 4.8250e-02, -8.4175e-02, -5.2390e-02,  9.7756e-03,  6.6823e-02],\n",
      "          [-6.7708e-04, -1.0034e-01, -8.1582e-02, -7.9765e-03,  2.1728e-02],\n",
      "          [ 2.8493e-02,  2.0940e-02,  1.1655e-01, -1.8213e-02,  4.5672e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4240e-01, -1.8317e-02,  1.6529e-02, -6.0321e-02, -1.5358e-01],\n",
      "          [ 2.0463e-02, -3.1464e-02, -5.3842e-02, -8.1468e-02, -1.6153e-01],\n",
      "          [ 2.9252e-02,  3.1919e-02, -6.5528e-02, -2.9258e-02, -1.1270e-01],\n",
      "          [ 1.1317e-03, -3.1987e-02, -5.7725e-02, -4.4091e-02, -1.1311e-01],\n",
      "          [-6.1787e-02, -8.3380e-02, -3.5238e-02,  7.5827e-02,  1.3088e-02]],\n",
      "\n",
      "         [[-9.6359e-02, -1.2662e-02,  5.6339e-02, -6.2170e-02, -3.5546e-02],\n",
      "          [-6.0765e-02, -3.3803e-03,  1.1407e-02,  3.6311e-02,  7.4813e-02],\n",
      "          [ 2.2508e-02,  3.0768e-02,  7.1403e-02,  8.7227e-02,  1.0323e-01],\n",
      "          [-1.1463e-02,  2.3200e-02,  9.2717e-02,  1.3123e-01,  2.8111e-02],\n",
      "          [ 3.3033e-02,  1.8197e-02,  5.4631e-02,  9.6221e-02,  1.4162e-02]],\n",
      "\n",
      "         [[ 1.0715e-01, -8.0993e-02, -9.1145e-02,  6.0800e-02,  6.6429e-02],\n",
      "          [ 9.4192e-02,  1.2219e-02, -6.1539e-02, -5.5848e-02,  1.4915e-02],\n",
      "          [-6.8437e-03, -6.2404e-02, -9.0540e-02, -7.0666e-02, -8.8520e-04],\n",
      "          [-2.3117e-02, -6.4031e-03,  1.2098e-03, -3.9840e-02,  9.1738e-02],\n",
      "          [-2.7788e-02, -7.0904e-02, -6.8155e-03,  4.5628e-02,  7.2520e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.4434e-02, -2.9985e-03, -7.9355e-03, -5.0803e-02,  4.5126e-02],\n",
      "          [-4.9696e-02,  6.9931e-02,  3.9170e-02,  3.7574e-02, -7.9815e-02],\n",
      "          [-4.8783e-02,  4.9305e-02, -1.7009e-02, -3.0840e-03, -2.3779e-02],\n",
      "          [ 1.8902e-02,  1.9456e-02,  9.8889e-04,  7.1122e-03, -2.8430e-02],\n",
      "          [ 4.0975e-02,  1.2968e-02, -2.2122e-02,  1.6037e-02, -5.0133e-02]],\n",
      "\n",
      "         [[ 2.9638e-02,  1.8581e-02, -1.6077e-01, -1.3908e-01,  3.5522e-02],\n",
      "          [ 8.8330e-02,  6.4899e-03, -9.0655e-02, -1.6959e-02,  7.9264e-02],\n",
      "          [ 2.1616e-02,  2.1756e-02,  4.2387e-02,  1.9684e-02,  2.5553e-02],\n",
      "          [-1.7427e-02,  1.1434e-02,  7.9824e-03, -4.3212e-02,  2.4924e-02],\n",
      "          [-3.9180e-02, -2.3848e-02,  1.2561e-02, -2.4680e-03, -4.4835e-02]],\n",
      "\n",
      "         [[-6.2483e-02,  5.8459e-02,  2.7663e-02,  3.2817e-02, -8.0309e-02],\n",
      "          [-2.2083e-02,  1.2619e-02,  5.8971e-02, -7.3291e-02, -5.3267e-02],\n",
      "          [ 3.0998e-02,  2.2886e-02, -5.8498e-02, -1.6923e-01, -4.9770e-02],\n",
      "          [-3.4483e-02, -4.6521e-02, -5.3305e-02, -9.1701e-02,  5.5555e-02],\n",
      "          [ 3.2862e-03, -4.4679e-02,  5.0848e-02,  4.3624e-03,  1.4171e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.9877e-03,  1.5932e-02, -5.2352e-02, -6.2825e-02, -1.2708e-01],\n",
      "          [-8.3262e-02,  3.6252e-02, -1.8821e-02, -1.1479e-01, -6.7980e-02],\n",
      "          [-7.9429e-03, -2.6039e-02, -1.4441e-02, -6.5822e-02, -4.8495e-02],\n",
      "          [-7.7689e-02, -1.0890e-01, -1.3721e-01, -1.3482e-01, -6.4765e-02],\n",
      "          [-7.4576e-02, -1.7044e-01, -1.7676e-01, -1.3115e-01, -9.1650e-02]],\n",
      "\n",
      "         [[ 1.5263e-01,  9.7694e-02,  1.1510e-02, -3.7193e-03, -1.3003e-03],\n",
      "          [ 6.9079e-02,  1.4152e-01,  1.7825e-02,  4.9400e-02,  5.2779e-02],\n",
      "          [ 3.4803e-02,  6.2726e-02,  6.0929e-03,  8.7247e-02,  2.7330e-02],\n",
      "          [ 6.9452e-02,  1.7344e-02,  8.3359e-02,  1.3326e-01,  1.3109e-02],\n",
      "          [ 1.1526e-01,  7.7346e-02,  6.7848e-02,  4.2444e-02,  8.2429e-03]],\n",
      "\n",
      "         [[-8.3803e-02, -6.0916e-02, -3.5917e-02, -7.7910e-02, -1.0648e-01],\n",
      "          [-1.7971e-01, -1.0490e-01, -4.6166e-02,  8.2799e-03, -1.2130e-01],\n",
      "          [-8.1833e-02, -2.9079e-03, -3.4017e-03, -2.7515e-02, -7.0612e-02],\n",
      "          [ 4.5092e-02, -7.9892e-02, -9.3917e-02, -6.5307e-02,  1.6895e-02],\n",
      "          [ 2.4198e-02, -6.3329e-02, -2.9126e-02, -1.2763e-03,  7.5300e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.4690e-02, -2.0268e-02,  3.3689e-02,  3.0131e-02, -3.2985e-02],\n",
      "          [ 8.7121e-03, -4.2494e-03,  4.6863e-02,  3.2389e-02,  8.7773e-02],\n",
      "          [-5.9011e-02,  1.4420e-02, -3.1348e-02,  6.7813e-02,  2.3163e-02],\n",
      "          [-1.3121e-02, -6.5172e-02, -5.8626e-02,  4.7233e-02,  3.6087e-03],\n",
      "          [ 5.3757e-02, -9.8162e-02, -2.9354e-02, -5.8918e-02,  1.1887e-02]],\n",
      "\n",
      "         [[-2.1720e-02, -4.3021e-02, -3.7089e-02, -2.1373e-02, -5.3795e-03],\n",
      "          [ 3.6728e-02, -4.3719e-02, -6.5447e-02, -6.8001e-02,  8.2164e-02],\n",
      "          [-1.2225e-02, -6.6298e-02, -1.1750e-01, -2.1008e-02,  8.8566e-02],\n",
      "          [ 2.6360e-02, -2.8045e-02, -5.1591e-02, -8.4328e-02,  2.7093e-03],\n",
      "          [ 1.8350e-02, -1.6205e-02, -2.6811e-03,  2.7067e-02,  3.5093e-02]],\n",
      "\n",
      "         [[-1.6684e-03,  5.5360e-02,  5.3234e-02, -5.3224e-02, -4.3731e-02],\n",
      "          [-9.9765e-02, -1.4053e-03, -3.7073e-02,  4.4931e-02, -1.6960e-02],\n",
      "          [-9.2427e-02,  3.9346e-02,  4.6681e-02,  1.6246e-03,  5.9784e-02],\n",
      "          [ 8.5512e-02,  9.5368e-02,  4.3250e-02,  2.7142e-02, -9.1135e-02],\n",
      "          [-3.2993e-03,  5.9804e-02, -6.4397e-03, -1.2217e-02, -3.7039e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.5491e-02,  2.1427e-02, -1.1108e-01, -1.2896e-01, -1.7948e-01],\n",
      "          [-1.2576e-01, -9.5512e-02, -5.6524e-02, -1.9947e-01, -1.4350e-01],\n",
      "          [-1.0108e-02, -1.0623e-01, -4.4575e-02, -9.1829e-02, -5.8882e-02],\n",
      "          [-4.4532e-02, -7.8560e-02, -4.1734e-02,  8.4598e-02,  1.3961e-01],\n",
      "          [-5.3181e-02, -6.9724e-02,  9.8463e-03,  7.3879e-02,  8.3970e-02]],\n",
      "\n",
      "         [[ 8.6202e-02,  8.7076e-02,  6.4688e-02,  1.0693e-01,  7.1516e-02],\n",
      "          [ 3.0637e-02,  8.7259e-02,  1.4831e-01,  1.6015e-01,  4.4636e-03],\n",
      "          [ 6.0183e-03,  3.5065e-03,  1.1766e-01,  1.0426e-01,  1.1558e-02],\n",
      "          [ 4.3605e-03, -4.3383e-03,  4.2234e-02, -2.0361e-02, -5.6524e-02],\n",
      "          [-2.0680e-03, -7.3098e-02, -8.5038e-02, -3.7059e-03, -6.1869e-03]],\n",
      "\n",
      "         [[-3.4890e-02, -7.5658e-02,  1.9243e-02, -4.5866e-02, -2.5477e-02],\n",
      "          [-1.3237e-01, -1.2773e-01, -9.0317e-02,  4.7380e-03, -2.6927e-02],\n",
      "          [-7.3230e-02, -4.4893e-02,  2.8680e-03,  2.0782e-02, -9.0720e-03],\n",
      "          [-1.4275e-02,  2.1612e-02,  5.4123e-02,  5.2708e-02,  5.4428e-02],\n",
      "          [ 9.5589e-02, -5.4738e-02, -1.1979e-02, -2.5803e-02, -3.2110e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.8067e-02,  3.9027e-02, -1.0366e-02,  9.9151e-03,  3.6263e-02],\n",
      "          [-7.2428e-02,  3.4239e-03, -2.8694e-02, -6.0325e-02,  6.3499e-03],\n",
      "          [-6.6043e-02,  3.3970e-02,  3.7595e-02,  5.7761e-02,  5.3372e-02],\n",
      "          [-9.1953e-02,  5.1157e-02, -2.3348e-03, -3.0395e-02, -6.3926e-02],\n",
      "          [-2.6756e-02,  3.5169e-02,  5.3145e-02,  4.5710e-02,  2.7998e-02]],\n",
      "\n",
      "         [[-3.9095e-02,  8.9405e-02,  1.0568e-01,  1.9690e-01,  1.1775e-01],\n",
      "          [-3.1580e-02,  1.0842e-02,  3.6928e-02,  1.2304e-01,  4.6411e-02],\n",
      "          [-4.3193e-02,  2.3655e-02, -6.9019e-02, -9.2526e-02, -5.0858e-02],\n",
      "          [-4.0484e-02, -1.4449e-01, -6.4228e-02, -1.5634e-02, -2.5966e-02],\n",
      "          [-8.4884e-02, -9.7374e-02, -5.7058e-02,  9.7188e-02,  4.2471e-03]],\n",
      "\n",
      "         [[-3.6659e-02, -5.4839e-02, -8.8104e-02, -5.2171e-02, -8.6741e-02],\n",
      "          [-4.4007e-02, -1.4786e-01, -6.7519e-02, -6.1244e-02,  2.0395e-02],\n",
      "          [-8.5091e-02, -1.5877e-02,  4.2662e-02,  6.2856e-03, -7.9217e-02],\n",
      "          [ 1.2711e-02,  7.7284e-02,  3.6498e-03, -4.9219e-03,  1.8434e-02],\n",
      "          [ 5.4456e-02,  1.1488e-01, -1.0941e-02, -7.7992e-02, -9.7794e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7940e-02, -5.7303e-02, -7.5634e-03, -2.7487e-02, -6.0798e-03],\n",
      "          [-1.3395e-01, -1.4183e-01, -6.2863e-02,  7.7471e-02,  1.5592e-01],\n",
      "          [-5.2425e-02, -7.4071e-02, -4.5365e-02,  1.8035e-02,  5.0631e-02],\n",
      "          [-8.0217e-02, -2.9053e-02, -2.7507e-02, -1.7782e-02, -2.5191e-02],\n",
      "          [-2.9678e-02,  2.0794e-02,  1.2912e-01,  8.3246e-02,  1.7864e-02]],\n",
      "\n",
      "         [[ 5.7225e-02, -5.8653e-02,  2.5977e-02,  1.1348e-01,  1.0640e-01],\n",
      "          [ 4.3415e-02, -3.9356e-02,  6.7945e-02,  9.2820e-03,  3.1888e-02],\n",
      "          [ 9.9807e-02,  5.3630e-02,  9.1304e-02, -4.4774e-02, -8.0303e-02],\n",
      "          [ 1.3220e-01,  3.8615e-02,  8.0869e-03, -1.0221e-01, -6.1428e-02],\n",
      "          [ 1.3510e-01,  2.3859e-02, -6.6032e-03, -1.3178e-01, -1.0426e-01]],\n",
      "\n",
      "         [[-1.5232e-02, -1.1015e-01, -1.2713e-01, -9.8535e-02, -3.9075e-02],\n",
      "          [-9.6137e-02, -1.7128e-01, -8.8370e-02, -5.5687e-02,  5.4126e-02],\n",
      "          [-8.6697e-02, -3.4552e-02,  3.9672e-02, -7.3914e-03,  5.0632e-02],\n",
      "          [-4.4268e-02,  3.3260e-03,  8.1190e-02, -1.9039e-02,  1.3274e-03],\n",
      "          [-4.0041e-02,  4.6867e-03,  2.0831e-02,  2.7287e-02, -7.5209e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.1703e-02, -1.6554e-03, -1.8812e-02, -1.5145e-02,  6.4217e-02],\n",
      "          [-1.8659e-02,  1.8369e-02, -2.2317e-02, -2.1233e-02,  1.3261e-02],\n",
      "          [ 1.6539e-02, -7.4718e-02,  4.8483e-02,  6.9619e-02,  2.7459e-02],\n",
      "          [-8.4667e-03, -3.1882e-03,  8.5043e-02,  7.3031e-03,  5.7466e-02],\n",
      "          [ 3.1655e-02, -2.0785e-02, -5.1700e-03, -1.9680e-02,  4.2683e-02]],\n",
      "\n",
      "         [[-1.0082e-02,  3.1079e-02,  5.3658e-02,  1.2033e-01,  5.2474e-02],\n",
      "          [-2.9047e-02,  6.2991e-02,  6.5429e-02,  2.1771e-02,  8.3356e-02],\n",
      "          [-8.4908e-03, -1.3185e-02, -8.0039e-02, -5.6089e-02,  7.0300e-02],\n",
      "          [ 2.4186e-02, -1.0154e-01, -1.8124e-01, -4.8141e-02,  5.9113e-02],\n",
      "          [-6.1978e-02, -6.7020e-02, -5.0530e-02, -3.5362e-02,  7.8125e-02]],\n",
      "\n",
      "         [[-4.0266e-02, -8.7514e-02, -6.7245e-02, -1.4361e-02, -3.9653e-02],\n",
      "          [-4.2240e-02, -6.3906e-02, -2.2376e-02, -3.8181e-02,  3.5962e-02],\n",
      "          [ 8.0362e-03,  2.1943e-02,  2.8144e-02,  9.4852e-02,  2.2184e-02],\n",
      "          [ 6.2621e-02,  1.5545e-01,  7.0823e-02,  9.8211e-02,  1.3199e-03],\n",
      "          [ 6.5644e-02,  3.9019e-02,  9.8594e-02,  2.8729e-02, -1.1521e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.5416e-02, -1.2678e-02, -2.1523e-02, -3.5038e-02, -1.0156e-01],\n",
      "          [-4.3301e-02, -7.3646e-02, -5.3766e-02, -1.0532e-01, -5.3619e-02],\n",
      "          [ 1.9002e-03,  8.2267e-03, -5.6359e-02, -3.7848e-02, -4.0018e-02],\n",
      "          [-7.3536e-02, -7.4250e-02,  3.1743e-03,  1.4889e-01,  1.4977e-01],\n",
      "          [-1.5557e-02, -4.0360e-02,  7.6084e-02,  1.8424e-01,  1.8879e-01]],\n",
      "\n",
      "         [[ 8.2476e-02,  3.7847e-02, -2.4267e-02, -2.2170e-02, -1.4330e-02],\n",
      "          [ 6.8010e-02,  8.1424e-02,  1.0874e-02,  1.0341e-01, -6.2281e-02],\n",
      "          [ 6.1606e-02,  6.2861e-02,  7.8862e-02,  1.6546e-02, -8.4995e-02],\n",
      "          [ 7.2428e-03,  9.4035e-02,  4.1819e-02,  4.1290e-02, -7.1546e-02],\n",
      "          [ 2.2267e-02, -7.1896e-02, -6.6610e-02,  2.4632e-02, -1.3786e-01]],\n",
      "\n",
      "         [[ 3.3979e-02, -1.0366e-01, -7.9228e-02,  3.0842e-02,  1.9450e-02],\n",
      "          [ 9.0624e-03, -1.1913e-01, -1.3882e-01, -5.1626e-02, -8.9737e-02],\n",
      "          [-4.0945e-02, -3.2075e-02, -8.2653e-02, -2.7591e-02,  2.4627e-02],\n",
      "          [-1.1566e-02,  1.3757e-02,  5.6353e-03,  1.2476e-02,  7.1830e-02],\n",
      "          [ 7.7808e-02, -3.2915e-02, -5.2136e-02,  1.0458e-01, -2.2104e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 7.3207e-04,  3.8382e-02, -3.0757e-02, -1.0904e-02, -2.0339e-02],\n",
      "          [-5.0761e-03,  2.6470e-02,  6.1868e-02,  2.3123e-02,  4.6244e-02],\n",
      "          [ 2.0672e-02,  5.4430e-02,  4.8869e-02,  8.5565e-03, -1.1226e-02],\n",
      "          [-1.1994e-01, -2.5828e-03,  4.3135e-02,  4.7911e-02, -4.6586e-02],\n",
      "          [-4.7193e-02,  2.3901e-02, -5.5450e-02, -6.9777e-03,  3.0467e-02]],\n",
      "\n",
      "         [[-1.6032e-02,  1.2026e-02,  3.3122e-02,  6.9359e-02,  6.4372e-03],\n",
      "          [-9.6467e-03, -8.6638e-02,  2.2855e-02,  3.7384e-02, -5.6526e-02],\n",
      "          [-1.3730e-02, -4.9877e-02, -4.0388e-02, -1.0192e-02, -2.3788e-02],\n",
      "          [-6.3344e-02,  1.2848e-02, -3.3376e-03, -6.5578e-02, -3.8308e-02],\n",
      "          [ 9.9627e-02,  1.1168e-01,  8.9444e-02,  1.7398e-02, -6.3002e-02]],\n",
      "\n",
      "         [[-5.6718e-02,  4.4462e-02, -5.1008e-02, -7.1808e-02,  4.1725e-02],\n",
      "          [ 5.9310e-02,  1.2181e-01,  3.6983e-02, -3.5714e-02, -3.3872e-02],\n",
      "          [ 4.8999e-02,  1.0055e-01,  2.2317e-02,  9.2335e-03,  2.8478e-03],\n",
      "          [-1.1810e-01, -4.4905e-02,  2.8573e-02,  2.6740e-03,  5.4495e-02],\n",
      "          [-7.5827e-02, -2.0377e-02, -3.7808e-02,  3.3731e-02,  1.0685e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.1457e-02, -9.1851e-02, -6.6280e-02,  6.7341e-02,  9.1806e-02],\n",
      "          [ 6.1292e-02,  3.5707e-02, -4.5820e-03, -8.8227e-03,  1.3035e-02],\n",
      "          [ 1.0807e-02,  2.1823e-02,  1.1491e-01,  2.7669e-02, -7.9970e-02],\n",
      "          [ 4.5071e-02,  3.9003e-02, -3.8458e-03, -1.4851e-02, -3.4021e-02],\n",
      "          [-5.4855e-02, -1.5814e-01, -1.2082e-01, -5.4918e-02, -1.1333e-01]],\n",
      "\n",
      "         [[-4.2400e-02, -2.8548e-02,  3.4130e-02, -2.8160e-02, -1.2320e-01],\n",
      "          [ 5.1053e-02, -1.9826e-03, -4.1054e-02, -8.7504e-02, -4.9406e-02],\n",
      "          [-7.9024e-03, -7.2754e-02,  3.9410e-02, -9.3114e-02, -2.8884e-02],\n",
      "          [-1.3916e-01, -6.6089e-02,  7.0255e-02,  3.6330e-03, -4.1543e-02],\n",
      "          [ 4.1008e-02,  4.5807e-03,  2.8804e-02,  2.8912e-02, -3.2730e-02]],\n",
      "\n",
      "         [[-3.5313e-03, -3.4677e-02, -5.1351e-02,  5.9626e-02,  6.1450e-02],\n",
      "          [-1.2357e-02,  9.1965e-02,  1.2273e-01, -5.8372e-03, -3.7753e-02],\n",
      "          [ 6.7921e-02,  1.3333e-01,  1.8063e-01,  7.5631e-02, -4.1684e-05],\n",
      "          [-5.6435e-02, -2.6666e-02,  2.6888e-02,  6.1145e-02,  6.2839e-02],\n",
      "          [-6.6601e-02, -7.8574e-02, -1.0971e-01, -4.7899e-02,  1.4826e-03]]]])\n",
      "torch.Size([20, 10, 5, 5])\n",
      "tensor([-0.0024,  0.0205, -0.0234, -0.0531, -0.0882, -0.0827,  0.0174,  0.0208,\n",
      "        -0.0515, -0.0447, -0.0213,  0.0701, -0.0124,  0.0008, -0.0549, -0.0800,\n",
      "        -0.0191, -0.0440,  0.0166,  0.0243])\n",
      "torch.Size([20])\n",
      "tensor([[-0.0561,  0.0013,  0.0545,  ..., -0.0026,  0.0596, -0.0087],\n",
      "        [-0.0271,  0.0425,  0.0718,  ...,  0.0636, -0.0016, -0.0347],\n",
      "        [-0.0010, -0.0464, -0.0221,  ..., -0.0282, -0.0009,  0.0833],\n",
      "        ...,\n",
      "        [-0.0338,  0.0560,  0.1180,  ..., -0.0069,  0.0103,  0.0619],\n",
      "        [ 0.0531, -0.0479, -0.0959,  ..., -0.0445, -0.0671,  0.0432],\n",
      "        [-0.0288,  0.0128,  0.1232,  ..., -0.0348,  0.0330, -0.0789]])\n",
      "torch.Size([50, 320])\n",
      "tensor([ 0.0015,  0.0729, -0.0093,  0.0836,  0.0876,  0.0365,  0.0009,  0.0126,\n",
      "         0.0340,  0.0547, -0.0143,  0.0467,  0.0500, -0.0163, -0.0425, -0.0058,\n",
      "        -0.0135, -0.0389,  0.0005,  0.0509, -0.0067,  0.0354,  0.1023, -0.0013,\n",
      "         0.0249, -0.0001,  0.0592,  0.0265,  0.0955,  0.0761,  0.0099,  0.0830,\n",
      "         0.0365,  0.0543, -0.0041, -0.0436,  0.0438,  0.0100,  0.0015, -0.0227,\n",
      "         0.0864,  0.0566, -0.0102,  0.0490,  0.0373,  0.0337,  0.0956, -0.0026,\n",
      "        -0.0020,  0.0814])\n",
      "torch.Size([50])\n",
      "tensor([[-0.2027,  0.2418,  0.2564, -0.0103, -0.1302,  0.0943, -0.1610, -0.0740,\n",
      "          0.1921, -0.1087, -0.2137, -0.2280, -0.3060,  0.2018, -0.2095, -0.2158,\n",
      "         -0.1279, -0.1323, -0.2404,  0.1659, -0.2429,  0.1145, -0.2335, -0.1986,\n",
      "          0.1843,  0.1239,  0.1586, -0.2608,  0.2005, -0.2688,  0.1581,  0.1780,\n",
      "         -0.1013, -0.2566, -0.1540,  0.1650,  0.1232,  0.1292,  0.1344,  0.1740,\n",
      "         -0.1605, -0.2456, -0.1484,  0.1154,  0.2325, -0.1443, -0.2701,  0.1131,\n",
      "          0.1762, -0.1784],\n",
      "        [-0.1660, -0.1727, -0.0909, -0.0864,  0.2049, -0.1661, -0.1908,  0.1422,\n",
      "          0.2382, -0.1017, -0.2013,  0.2663,  0.1479, -0.1253,  0.0681, -0.1845,\n",
      "          0.2351, -0.0988,  0.1976, -0.1555, -0.1797, -0.2642,  0.1908, -0.2209,\n",
      "         -0.1382, -0.1274, -0.1588, -0.2493,  0.2286,  0.1820, -0.1662,  0.2078,\n",
      "         -0.1912,  0.1577,  0.2278, -0.0186,  0.0080,  0.1426, -0.2060, -0.1460,\n",
      "          0.1674, -0.2145, -0.2478,  0.0795, -0.2454,  0.0285,  0.2520, -0.1437,\n",
      "         -0.1768,  0.1784],\n",
      "        [ 0.1788, -0.2087,  0.2461,  0.2074,  0.1907,  0.0413, -0.0455, -0.1040,\n",
      "          0.2058, -0.1261, -0.1978,  0.0232,  0.1519, -0.1714, -0.0500,  0.1650,\n",
      "          0.2090,  0.2185, -0.1227, -0.0707, -0.1785,  0.0331, -0.2082, -0.1761,\n",
      "          0.1598,  0.1807, -0.0896,  0.1613,  0.1682,  0.1912, -0.1468, -0.0823,\n",
      "         -0.0476, -0.2629,  0.1991,  0.1094,  0.1134, -0.2115, -0.0855, -0.2209,\n",
      "         -0.1614,  0.0137,  0.0232,  0.1393, -0.0031, -0.0970, -0.0569,  0.1092,\n",
      "          0.2125, -0.1701],\n",
      "        [ 0.1899,  0.1590, -0.1116,  0.2364, -0.1924,  0.1552,  0.0076, -0.1988,\n",
      "         -0.1647, -0.0170, -0.1169, -0.1392,  0.1435, -0.1353, -0.1735,  0.1800,\n",
      "          0.2261, -0.0574, -0.2341, -0.1110, -0.0050,  0.1304,  0.2030,  0.2127,\n",
      "         -0.1552,  0.1398,  0.2481,  0.0530, -0.0539,  0.0649, -0.1951, -0.1310,\n",
      "         -0.0694,  0.1699,  0.0651, -0.2151, -0.1989, -0.2523, -0.2099, -0.2136,\n",
      "         -0.2113, -0.0793, -0.1413,  0.1076, -0.0342,  0.2832, -0.1591,  0.0729,\n",
      "         -0.0270,  0.1892],\n",
      "        [-0.0451, -0.1167,  0.0262, -0.0212, -0.0733, -0.2266,  0.2994,  0.1769,\n",
      "          0.1365, -0.1932,  0.1615,  0.2927, -0.0730,  0.2448,  0.1904, -0.2104,\n",
      "         -0.1172, -0.0594,  0.2031, -0.1104,  0.1492, -0.3340, -0.0974, -0.1410,\n",
      "          0.0969, -0.2004, -0.1326,  0.2015, -0.1419, -0.1759,  0.1362, -0.1765,\n",
      "         -0.0050,  0.1260,  0.1906, -0.0006,  0.1437, -0.1610,  0.2064, -0.1594,\n",
      "         -0.1299,  0.1062,  0.3667, -0.2116, -0.1649, -0.0245, -0.1224,  0.1146,\n",
      "         -0.0955,  0.2158],\n",
      "        [-0.1174,  0.2461, -0.0934, -0.1424, -0.0371,  0.1625, -0.1957, -0.1663,\n",
      "         -0.0968,  0.2488,  0.1476, -0.1524, -0.2464,  0.0267, -0.2448,  0.1557,\n",
      "         -0.0540, -0.0631, -0.0619, -0.0287,  0.1345,  0.1113,  0.1875,  0.2288,\n",
      "         -0.1536, -0.1881,  0.2506,  0.0059, -0.1047, -0.2094,  0.1487, -0.0814,\n",
      "         -0.1680,  0.1937,  0.0383,  0.1616,  0.0983,  0.0518,  0.1995,  0.1690,\n",
      "          0.1632, -0.1153, -0.0629, -0.3051,  0.2061,  0.2491,  0.0052, -0.2074,\n",
      "         -0.1904, -0.0208],\n",
      "        [ 0.2029,  0.2466, -0.0114, -0.2293,  0.1705, -0.2607, -0.2325, -0.0648,\n",
      "          0.2199,  0.2969,  0.1955, -0.1794, -0.2579, -0.2021, -0.2792, -0.1896,\n",
      "         -0.2007,  0.0279,  0.1820,  0.1313, -0.2687,  0.0563, -0.1247,  0.0232,\n",
      "          0.1872, -0.4011, -0.0858, -0.2409, -0.1386, -0.2544,  0.2114, -0.1188,\n",
      "         -0.0798, -0.1917,  0.1778,  0.1649,  0.1643,  0.0278,  0.2494,  0.2064,\n",
      "          0.1935, -0.1834,  0.0035, -0.2624, -0.0802, -0.2150, -0.1781,  0.0921,\n",
      "         -0.1959, -0.2161],\n",
      "        [ 0.0937, -0.1033, -0.0087,  0.2227, -0.1603,  0.1211,  0.2660,  0.1274,\n",
      "         -0.2382, -0.1679, -0.2750,  0.2917,  0.1562,  0.1660, -0.0424,  0.1429,\n",
      "          0.1870,  0.2315, -0.2292, -0.1130,  0.1573, -0.2800, -0.2565, -0.2336,\n",
      "         -0.2411,  0.1649, -0.1614, -0.1386, -0.1262,  0.1429, -0.1502,  0.1559,\n",
      "          0.2566,  0.0931, -0.0354, -0.1956, -0.2926,  0.1276,  0.0714, -0.0567,\n",
      "         -0.2923,  0.2064,  0.0110,  0.1077,  0.2390, -0.1306,  0.2376, -0.2629,\n",
      "          0.1285, -0.0381],\n",
      "        [ 0.0598, -0.1026, -0.1258, -0.0361,  0.1924, -0.1885, -0.0823, -0.1046,\n",
      "         -0.1147,  0.2450,  0.1193, -0.1199,  0.0913, -0.0252,  0.1777,  0.1439,\n",
      "         -0.1446, -0.1071, -0.0470,  0.0419, -0.1419,  0.1052,  0.1202,  0.1812,\n",
      "         -0.1217,  0.1641, -0.1057,  0.1667, -0.1831,  0.1267,  0.1588, -0.0905,\n",
      "         -0.1129,  0.1499, -0.0114,  0.1468,  0.1551,  0.1330, -0.1784,  0.0907,\n",
      "          0.1667,  0.2537,  0.1006,  0.1385, -0.0351, -0.1414, -0.1813, -0.0763,\n",
      "          0.2223, -0.2002],\n",
      "        [-0.2662, -0.1388,  0.1155, -0.1483, -0.2377,  0.0461,  0.2411,  0.0811,\n",
      "         -0.1017, -0.1368,  0.1422,  0.2399, -0.2249,  0.1996,  0.0373,  0.1708,\n",
      "         -0.1420, -0.0318,  0.1321,  0.0505,  0.1288,  0.0724,  0.0723,  0.2039,\n",
      "          0.1011,  0.1431, -0.0149,  0.1731, -0.0995, -0.1720, -0.1498, -0.1506,\n",
      "          0.2021,  0.1828, -0.2985, -0.0703, -0.1838, -0.1546,  0.0213,  0.1629,\n",
      "         -0.0761,  0.2385,  0.3339,  0.1226,  0.2263,  0.2497, -0.1326,  0.0057,\n",
      "         -0.1265,  0.1274]])\n",
      "torch.Size([10, 50])\n",
      "tensor([-0.0475,  0.3690,  0.0153,  0.1802,  0.0982, -0.1489, -0.1172,  0.1190,\n",
      "         0.0581, -0.1445])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in model_torch.parameters():\n",
    "    print(param.data)\n",
    "    print(param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "imported = tf.saved_model.load('mnist.pb')\n",
    "f = imported.signatures[\"serving_default\"]\n",
    "#print(f(x=tf.constant([[1.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       " array([[-1828.0537 , -1347.0143 , -1056.4309 ,     0.     , -1870.2682 ,\n",
       "         -1012.452  , -2525.101  , -1003.25433, -1007.50964,  -741.2178 ]],\n",
       "       dtype=float32)>}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(tf.convert_to_tensor(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "model_new = tf.keras.models.load_model('mnist.pb')\n",
    "f_new = model_new.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       " array([[-1828.0537 , -1347.0143 , -1056.4309 ,     0.     , -1870.2682 ,\n",
       "         -1012.452  , -2525.101  , -1003.25433, -1007.50964,  -741.2178 ]],\n",
       "       dtype=float32)>}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_new(tf.convert_to_tensor(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-50f4986bbbd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         np_config.enable_numpy_behavior()\"\"\".format(type(self).__name__, name))\n\u001b[1;32m--> 513\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "print(type(f_new.inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "cuda = torch.device('cuda')\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/lzw365/Downloads/DeepEverest/test.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000018vscode-remote?line=14'>15</a>\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_relu_stack(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000018vscode-remote?line=15'>16</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m logits\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000018vscode-remote?line=17'>18</a>\u001b[0m model \u001b[39m=\u001b[39m NeuralNetwork()\u001b[39m.\u001b[39;49mto(cuda)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000018vscode-remote?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=902'>903</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=903'>904</a>\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=904'>905</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=906'>907</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=596'>597</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=597'>598</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=598'>599</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=599'>600</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=600'>601</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=601'>602</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=602'>603</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=901'>902</a>\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=902'>903</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=903'>904</a>\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/nn/modules/module.py?line=904'>905</a>\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:216\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/cuda/__init__.py?line=211'>212</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/cuda/__init__.py?line=212'>213</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/cuda/__init__.py?line=213'>214</a>\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/cuda/__init__.py?line=214'>215</a>\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/cuda/__init__.py?line=215'>216</a>\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/cuda/__init__.py?line=216'>217</a>\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/cuda/__init__.py?line=217'>218</a>\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/cuda/__init__.py?line=218'>219</a>\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.10/site-packages/torch/cuda/__init__.py?line=219'>220</a>\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(cuda)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 10)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 10)\n",
    "        self.fc1 = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(self.fc(x)))\n",
    "        return x\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fc\n",
      "fc1\n",
      "relu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0813]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "# out of place\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "d = dict(model.named_modules())\n",
    "\n",
    "for i in d:\n",
    "    print(i)\n",
    "    model.fc1.register_forward_hook(get_activation(i))\n",
    "\n",
    "model.fc.register_forward_hook(get_activation('fc'))\n",
    "output = model(x)\n",
    "activation['fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7ff8fd9f2b90>\n",
      "fc\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7ff8fd9f09d0>\n",
      "fc1\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7ff8fd9f10f0>\n",
      "relu\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7ff8fd9f1cf0>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lzw365/Downloads/DeepEverest/test.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000023vscode-remote?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(hooks[name])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000023vscode-remote?line=12'>13</a>\u001b[0m lists \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mnamed_modules()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000023vscode-remote?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(lists[\u001b[39m2\u001b[39;49m][\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "hooks = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        hooks[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n",
    "    hooks[name] = module.register_forward_hook(get_activation(name))\n",
    "    #output = model(x)\n",
    "    print(hooks[name])\n",
    "lists = model.named_modules()\n",
    "print(lists[2][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3853,  0.8042,  1.2120, -0.5853,  0.2316, -1.4203, -0.0124,  1.0610,\n",
      "          0.2698, -1.4225]])\n",
      "tensor([[-1.0813]])\n"
     ]
    }
   ],
   "source": [
    "output = model(x)\n",
    "print(hooks['fc'])\n",
    "output = model(x)\n",
    "print(hooks['fc1'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8e91a9fb4a18180c8edf2062f1258c709cf2408f146570f2930cfcc1d509a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
