{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(320, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n",
    "                    epoch,  loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the maxlog-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])), batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, \n",
    "                        transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])), batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Net(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): Dropout2d(p=0.5, inplace=False)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): ReLU()\n",
      "    (7): Flatten(start_dim=1, end_dim=-1)\n",
      "    (8): Linear(in_features=320, out_features=50, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Dropout(p=0.5, inplace=False)\n",
      "    (11): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (12): Softmax(dim=None)\n",
      "  )\n",
      ")\n",
      "Train Epoch: 0 \tLoss: -0.099253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzw365/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: -0.1837, Accuracy: 3251/10000 (33%)\n",
      "\n",
      "Train Epoch: 1 \tLoss: -0.150705\n",
      "\n",
      "Test set: Average loss: -0.7915, Accuracy: 8050/10000 (80%)\n",
      "\n",
      "Train Epoch: 2 \tLoss: -0.522352\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lzw365/Downloads/DeepEverest/test.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000005vscode-remote?line=4'>5</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model_torch\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m     train(model_torch, device, train_loader, optimizer, epoch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000005vscode-remote?line=7'>8</a>\u001b[0m     test(model_torch, device, test_loader)\n",
      "\u001b[1;32m/home/lzw365/Downloads/DeepEverest/test.ipynb Cell 4'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000003vscode-remote?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000003vscode-remote?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(output, target)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000003vscode-remote?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000003vscode-remote?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223130302e36342e37312e3136222c2275736572223a226c7a77333635227d/home/lzw365/Downloads/DeepEverest/test.ipynb#ch0000003vscode-remote?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/lzw365/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "model_torch = Net().to(device)\n",
    "print(model_torch)\n",
    "optimizer = optim.SGD(model_torch.parameters(), lr=0.01, momentum=0.5)\n",
    "for epoch in range(10):\n",
    "    train(model_torch, device, train_loader, optimizer, epoch)\n",
    "    test(model_torch, device, test_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of Net(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_torch.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_torch.state_dict(), 'mnist.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_torch = Net()\n",
    "model_torch.load_state_dict(torch.load('mnist.pth'))\n",
    "dummy_input = Variable(torch.randn(1, 1, 28, 28))\n",
    "torch.onnx.export(model_torch.eval(), dummy_input, \"mnist.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 20:39:33.720975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 20:39:33.727853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-25 20:39:33.727875: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-25 20:39:33.728301: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_onnx = onnx.load('mnist.onnx')\n",
    "model_tf = prepare(model_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAAnRSTlMAJqSeSMUAAADnSURBVHic1ZAhawNBEIUfpYksl7qDmCM6tGNDRSHU7E/oTzi7pH8i4nRs/ZpAIPbM/YOIJVB7DVkzUaUEXlQKt711NX1qmI+ZN2+AP9PE+ySrWm4SKG/IJu9n4kg7Tww+kfqauuWgNKlj3tQl/ICavuh2bn4q84j1VxfeXovs5e702QKw4/MiXprVpADWHknG1ob0U4gj1bPu0qzkYYniPRydzOjzaFAtMketBtOdrqKMJNDw22CuXCKGDrKnReFDGUcZnYDzEA/+/uO5jZOIqpSBZNvzXQmqJBmq3wwwWwauSulj/0MXsmhrvbrd5VAAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F5B069E2AF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n",
      "The digit is classified as [[-1050.928  -1503.1975 -1432.3577 -1705.7224 -1070.0522 -1097.5026\n",
      "      0.     -1961.2129 -1064.8805 -1536.6558]] pytorch\n",
      "The digit is classified as Outputs(_0=array([[-1050.928 , -1503.1974, -1432.3577, -1705.7225, -1070.0521,\n",
      "        -1097.5027,     0.    , -1961.213 , -1064.8806, -1536.6558]],\n",
      "      dtype=float32)) tf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Image 1:')\n",
    "img = Image.open('two.png').resize((28, 28)).convert('L')\n",
    "display(img)\n",
    "temp = np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :]\n",
    "print(temp.shape)\n",
    "output = model_torch(torch.from_numpy(temp))\n",
    "print('The digit is classified as', output.detach().numpy(), 'pytorch')\n",
    "output = model_tf.run(temp)\n",
    "print('The digit is classified as', output, 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 2:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABf0lEQVR4nGNgoD9gxBBg5GX4+QNDkpmBQ5BJXIjLg+HattsMDAwMDCxwHULyzHpRrNJSTKz/f+qWfEAxTnbjoyfvfv/5/fPLu89/XzexIBvLaLuZ7/+/rx8fPLhzU7OY54bRdyRj/z86p/3ux649l9/8+6eZysOKaufjYNZ//3/8/M3AIBDKx/DjP1a/sUhVvfj/towZRScDA4uOyOc3HAZR9lzv1878i+Z7iUXfPh+5+v3P/1fTlNHNY3d/9v//n7///j6plGXGCDfJaQeePH3371M+B9wiuOT/5/nc+mwuhf9//UKWZJF8/YOBgYHh94ejrEr/WaWZ/0ElmRgYWK2W+UN5f9itWX6eRnapzNbPM/kgwchi9/z/PWFkl6jc+vvpiBcvCyu3Zv75v5dDWZAlOX3O/vr/cmFiyuyLX//f8kXzBpP6mi///3z9+vvft6uZrBhetF726ufPn9+fzrNFkYM4hFVE3Zv137NDN7/+wZRkYGBkZ2D494uBWgAANqOZcFTmPBAAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F5B2BF50160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n",
      "The digit is classified as [[-1674.5397  -1686.7819   -955.34235     0.      -1886.2817  -1205.1274\n",
      "  -2681.6333  -1189.669    -993.9929   -944.36743]] pytorch\n",
      "The digit is classified as Outputs(_0=array([[-1674.5398 , -1686.7817 ,  -955.3423 ,     0.     , -1886.2817 ,\n",
      "        -1205.1274 , -2681.6333 , -1189.6688 ,  -993.9929 ,  -944.36743]],\n",
      "      dtype=float32)) tf\n"
     ]
    }
   ],
   "source": [
    "print('Image 2:')\n",
    "img = Image.open('three.png').resize((28, 28)).convert('L')\n",
    "display(img)\n",
    "temp = np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :]\n",
    "print(temp.shape)\n",
    "output = model_torch(torch.from_numpy(temp))\n",
    "print('The digit is classified as', output.detach().numpy(), 'pytorch')\n",
    "output = model_tf.run(temp)\n",
    "print('The digit is classified as', output, 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `__call__` contains input name(s) input.1 with unsupported characters which will be renamed to input_1 in the SavedModel.\n",
      "2022-04-25 20:39:52.598284: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist.pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist.pb/assets\n"
     ]
    }
   ],
   "source": [
    "model_tf.export_graph('mnist.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.3833,  0.0220, -0.0965,  0.1975,  0.2135],\n",
      "          [-0.3189, -0.1862,  0.0243,  0.4145,  0.2126],\n",
      "          [-0.1882,  0.1057,  0.3692,  0.3993,  0.1196],\n",
      "          [ 0.1749,  0.1542,  0.2534,  0.0729,  0.0680],\n",
      "          [ 0.1706,  0.0412, -0.1490,  0.1393, -0.1932]]],\n",
      "\n",
      "\n",
      "        [[[-0.0095, -0.1795,  0.0275, -0.1768, -0.1461],\n",
      "          [ 0.3106, -0.2124, -0.1373,  0.1904, -0.0599],\n",
      "          [ 0.3169,  0.0625, -0.1395,  0.4066,  0.3856],\n",
      "          [ 0.1472,  0.1810, -0.0762,  0.0525,  0.2633],\n",
      "          [ 0.0116,  0.0064, -0.3337, -0.2542, -0.0572]]],\n",
      "\n",
      "\n",
      "        [[[-0.0179,  0.1203,  0.0657,  0.2108, -0.0585],\n",
      "          [ 0.1656,  0.0551, -0.0301,  0.2053, -0.0580],\n",
      "          [-0.0362,  0.0716,  0.2244,  0.0163, -0.1420],\n",
      "          [ 0.2148, -0.0888,  0.1408, -0.0614, -0.2482],\n",
      "          [ 0.2197,  0.0198, -0.1216, -0.2027, -0.1967]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2603,  0.0345, -0.0561, -0.1583, -0.2309],\n",
      "          [ 0.1057,  0.2851, -0.2063, -0.1851, -0.2932],\n",
      "          [ 0.3078,  0.1645, -0.2470, -0.0689, -0.3293],\n",
      "          [ 0.2963,  0.1336, -0.1735, -0.3271,  0.0127],\n",
      "          [ 0.3783,  0.0584, -0.2108, -0.1454,  0.0706]]],\n",
      "\n",
      "\n",
      "        [[[-0.0850, -0.4331, -0.2640, -0.1002, -0.1260],\n",
      "          [-0.1744, -0.3002, -0.3488, -0.2297, -0.0576],\n",
      "          [ 0.4027,  0.3677,  0.2534, -0.1125, -0.1918],\n",
      "          [ 0.3227,  0.4163,  0.0705,  0.2381, -0.0454],\n",
      "          [-0.2092, -0.0808,  0.2119,  0.2463,  0.2483]]],\n",
      "\n",
      "\n",
      "        [[[-0.0759, -0.2181,  0.0748,  0.1863,  0.3323],\n",
      "          [-0.1774, -0.1962, -0.1978,  0.2377,  0.4101],\n",
      "          [-0.3142, -0.2387, -0.2118,  0.2255,  0.3546],\n",
      "          [-0.1639, -0.2545, -0.0597,  0.1050,  0.4689],\n",
      "          [-0.3417, -0.3041,  0.1102,  0.0821,  0.3706]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2132,  0.0679,  0.2770, -0.1178, -0.0589],\n",
      "          [-0.2102, -0.0339, -0.1987, -0.2472, -0.3139],\n",
      "          [-0.1664, -0.1453, -0.2545, -0.1747, -0.1994],\n",
      "          [-0.0668, -0.0148, -0.0501, -0.0797, -0.0239],\n",
      "          [-0.2005, -0.2437, -0.1462, -0.0314,  0.4252]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1955,  0.1294,  0.3427,  0.2120, -0.1559],\n",
      "          [-0.1723,  0.0644,  0.3593,  0.3258, -0.2439],\n",
      "          [-0.2234, -0.2465,  0.1877,  0.4113,  0.3099],\n",
      "          [-0.1658, -0.2525, -0.0732,  0.2002,  0.3670],\n",
      "          [-0.0941, -0.3866, -0.2756, -0.3987,  0.0847]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1571, -0.0368, -0.1193, -0.3150, -0.0355],\n",
      "          [ 0.0801, -0.2947, -0.2393, -0.2140,  0.1125],\n",
      "          [-0.0149,  0.0053, -0.2757,  0.2671,  0.2739],\n",
      "          [ 0.1253,  0.1830,  0.3353,  0.3258,  0.1936],\n",
      "          [ 0.1003,  0.2557,  0.1529, -0.0394, -0.1066]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0704, -0.2766, -0.0675,  0.0089, -0.1565],\n",
      "          [ 0.0187, -0.2437, -0.1038,  0.1184,  0.3573],\n",
      "          [ 0.2469,  0.4307,  0.4734,  0.1132,  0.1575],\n",
      "          [ 0.0202,  0.0504, -0.0271,  0.0339, -0.2234],\n",
      "          [-0.1750, -0.2981, -0.2097, -0.3081, -0.1493]]]])\n",
      "torch.Size([10, 1, 5, 5])\n",
      "tensor([ 0.1239,  0.0723,  0.0164, -0.1794,  0.1390, -0.0707,  0.2141, -0.1205,\n",
      "         0.1662,  0.0317])\n",
      "torch.Size([10])\n",
      "tensor([[[[ 7.7223e-02,  9.2267e-02,  4.0122e-02,  5.6616e-02,  1.2599e-02],\n",
      "          [ 9.7851e-02,  8.9738e-02, -3.0455e-04, -2.1386e-02, -6.6514e-03],\n",
      "          [-2.4268e-02, -3.5756e-02, -4.9468e-02, -4.3522e-02, -2.9774e-02],\n",
      "          [-1.8100e-02,  1.3775e-02, -6.7591e-02, -1.0948e-01, -8.1408e-02],\n",
      "          [-2.6057e-02,  4.7927e-02, -3.1200e-02, -8.8230e-03, -9.2860e-02]],\n",
      "\n",
      "         [[ 6.4468e-02,  1.0352e-01,  1.1085e-01,  8.1924e-02,  2.7613e-02],\n",
      "          [ 8.6063e-03, -2.8973e-02,  5.7485e-02,  4.3310e-02,  4.0746e-02],\n",
      "          [-6.9113e-03,  3.0009e-02,  2.5659e-02, -1.3857e-02, -1.1530e-02],\n",
      "          [ 1.6278e-02,  7.1800e-03,  7.5558e-02, -5.2758e-02, -1.0218e-01],\n",
      "          [-2.9397e-02, -2.1199e-02,  6.6159e-03, -1.9074e-02, -4.6709e-02]],\n",
      "\n",
      "         [[ 4.1176e-02, -2.7372e-02,  7.4292e-02, -1.6731e-02, -4.3288e-02],\n",
      "          [-1.1196e-02, -5.0522e-03,  4.1001e-02,  8.7019e-02,  2.6397e-02],\n",
      "          [-5.0214e-02, -7.7985e-02, -3.2833e-02,  8.0507e-02, -2.2135e-02],\n",
      "          [ 1.2779e-02, -5.0807e-02, -2.8293e-02,  2.4279e-02, -4.9144e-02],\n",
      "          [ 2.7198e-02, -2.5349e-02, -5.3745e-02, -2.6530e-02, -1.5389e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.1307e-02, -2.0774e-02, -8.9027e-02, -1.6636e-02, -2.2892e-02],\n",
      "          [ 2.1571e-02,  4.5454e-02, -8.9272e-02, -1.2510e-03, -2.5336e-02],\n",
      "          [ 7.0826e-02,  5.3494e-02, -6.4982e-02, -1.4976e-02, -1.5444e-02],\n",
      "          [ 1.4237e-01,  1.5409e-01,  4.7729e-02, -9.3363e-02, -8.6655e-03],\n",
      "          [-1.4290e-02,  1.1152e-01,  6.2080e-02,  4.9002e-02, -2.6825e-02]],\n",
      "\n",
      "         [[-2.9335e-02,  7.4242e-02, -2.9578e-02, -5.9316e-02,  4.2191e-02],\n",
      "          [-2.2933e-02,  4.4422e-03, -4.0143e-02,  2.4792e-02, -2.3267e-03],\n",
      "          [-7.6343e-02, -4.3440e-02,  1.2500e-01,  4.1525e-02, -8.0137e-02],\n",
      "          [-1.4770e-01,  1.0462e-02,  9.3361e-02,  7.3348e-02, -4.5388e-02],\n",
      "          [-2.9779e-03, -3.5700e-02, -8.2401e-02, -2.5936e-02, -7.0816e-03]],\n",
      "\n",
      "         [[ 7.4806e-03,  9.0752e-02,  5.1996e-02,  3.5663e-02, -1.1731e-02],\n",
      "          [-1.0656e-01, -7.9916e-02,  6.1703e-03,  1.3696e-02,  5.8183e-02],\n",
      "          [-9.4093e-02,  1.5080e-02,  4.1366e-02, -2.8471e-02, -2.8160e-02],\n",
      "          [-4.2599e-02,  2.6919e-02, -3.8177e-02,  6.8929e-03, -4.7448e-03],\n",
      "          [-3.1788e-02, -1.5198e-02,  7.1185e-02, -3.5727e-02, -5.5750e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.7904e-02, -8.9249e-02,  2.1112e-03,  6.6061e-02,  6.0626e-02],\n",
      "          [-2.4821e-02, -2.3771e-02,  9.4261e-02,  1.9328e-01,  1.2331e-01],\n",
      "          [-1.7439e-01, -1.2464e-01, -3.6392e-02, -2.0877e-02,  2.8038e-03],\n",
      "          [-1.5279e-01, -1.1262e-01, -2.1554e-01, -5.9738e-02,  6.6386e-02],\n",
      "          [-5.4775e-02, -1.2273e-01, -2.3311e-02,  6.4453e-02, -6.0604e-02]],\n",
      "\n",
      "         [[ 8.4234e-02,  8.6134e-03, -2.8659e-02,  3.5802e-02,  1.8194e-03],\n",
      "          [ 1.3336e-01,  2.7649e-02,  1.1966e-01,  2.1620e-02,  7.4087e-02],\n",
      "          [ 8.5945e-02,  5.1222e-02, -2.8586e-02, -2.9330e-02, -2.9284e-02],\n",
      "          [-7.4241e-02, -1.0277e-01, -1.1378e-01, -3.2484e-02, -1.2864e-01],\n",
      "          [ 9.9070e-02, -5.3622e-02,  1.0176e-01,  1.3025e-02,  5.9511e-02]],\n",
      "\n",
      "         [[-4.0447e-03, -5.1639e-02, -9.2887e-04, -2.3693e-02,  1.3531e-02],\n",
      "          [ 1.5632e-02, -2.8873e-02,  4.9670e-03,  2.1729e-02,  3.1488e-03],\n",
      "          [-3.1099e-02, -4.0922e-02,  2.7928e-02,  3.2239e-02, -4.0185e-02],\n",
      "          [-3.0903e-02, -1.5795e-03,  3.8338e-02, -2.0515e-02, -2.7876e-02],\n",
      "          [ 6.9917e-02, -2.4988e-02,  9.2338e-03, -3.1402e-02, -1.4230e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.3893e-02,  2.2989e-02,  6.5387e-02, -6.6485e-02, -9.0645e-02],\n",
      "          [ 1.0381e-01,  3.9969e-02,  4.4914e-02, -6.9979e-02, -9.8221e-02],\n",
      "          [ 3.2614e-02,  9.5382e-02,  5.6459e-02, -9.8334e-02, -1.3017e-01],\n",
      "          [-1.2353e-01, -8.4819e-02, -7.5612e-02, -7.4198e-02, -6.8797e-02],\n",
      "          [-1.3446e-01, -5.8410e-02, -5.7498e-02, -2.2930e-02,  5.5437e-02]],\n",
      "\n",
      "         [[-3.1111e-02,  6.8482e-02,  9.1693e-03,  1.3469e-01,  1.3556e-01],\n",
      "          [ 4.3074e-02, -2.7915e-02,  8.8322e-02,  1.2897e-01,  5.8271e-02],\n",
      "          [-1.6059e-01, -1.0804e-01, -3.4948e-02, -5.1406e-02, -3.2100e-02],\n",
      "          [-6.7157e-02, -8.6366e-02,  6.0660e-02,  3.0560e-02, -4.0638e-02],\n",
      "          [-6.6519e-02, -5.6170e-02,  4.0784e-02,  1.1610e-01, -3.3476e-02]],\n",
      "\n",
      "         [[-8.5809e-02,  2.4436e-02, -2.0823e-02, -2.7256e-02,  4.6849e-02],\n",
      "          [ 7.3716e-02,  1.6180e-02,  1.1981e-01,  1.1349e-01,  6.7327e-02],\n",
      "          [-5.0170e-02,  3.4532e-02,  4.5074e-02,  1.2740e-01, -4.7856e-02],\n",
      "          [ 2.6736e-02, -2.6655e-02, -5.6473e-02, -4.8082e-02,  2.8017e-02],\n",
      "          [ 8.5513e-02, -8.3585e-03,  4.7721e-02, -1.0026e-02,  2.2229e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.8278e-03,  1.3285e-01, -3.4469e-02, -3.6376e-02, -1.4084e-01],\n",
      "          [ 2.1581e-02, -7.6681e-04, -2.0279e-02, -3.7894e-02, -7.3098e-02],\n",
      "          [ 1.6051e-02, -1.5121e-02,  1.3391e-02,  1.4450e-02,  8.0676e-02],\n",
      "          [-1.5163e-01,  5.5178e-02,  1.3116e-01,  1.2978e-01,  8.6625e-02],\n",
      "          [-2.8543e-02,  8.9178e-03,  2.1384e-02, -6.7382e-04,  3.7352e-02]],\n",
      "\n",
      "         [[-9.5981e-02, -2.9819e-02,  5.2627e-02,  6.2702e-02,  3.4516e-02],\n",
      "          [-1.6253e-02,  2.5115e-02, -5.9084e-02, -4.2494e-02, -5.6154e-02],\n",
      "          [ 1.0866e-01,  4.3174e-02, -5.4783e-02, -1.3228e-02, -9.1539e-03],\n",
      "          [-2.3549e-02,  9.7746e-02,  3.7677e-02, -6.2840e-02, -7.7207e-02],\n",
      "          [-1.1066e-02,  5.8374e-02, -7.0407e-03, -1.1331e-01, -4.6092e-03]],\n",
      "\n",
      "         [[ 3.1950e-02, -2.6233e-02,  2.2251e-02,  9.0611e-04, -8.4058e-02],\n",
      "          [-7.2237e-02,  1.4982e-02, -1.1532e-01, -4.4256e-02,  5.6482e-02],\n",
      "          [-2.5969e-02, -5.2386e-02,  5.7525e-02,  5.5500e-02,  9.5831e-02],\n",
      "          [-6.6543e-02,  5.9691e-02,  4.8193e-02,  2.4939e-02,  6.1564e-02],\n",
      "          [-1.8608e-02, -3.5552e-03, -4.2449e-02,  5.1496e-02,  8.3851e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.6533e-02, -2.6544e-02,  2.7061e-02,  1.8150e-01,  1.5586e-01],\n",
      "          [ 3.8986e-02,  8.5710e-02,  7.9735e-02,  3.4688e-02,  1.5845e-01],\n",
      "          [ 1.1354e-01,  9.2453e-02, -3.2902e-02, -3.3462e-02, -4.5060e-02],\n",
      "          [ 1.2788e-01, -1.0010e-02, -8.6747e-02, -1.9590e-02, -1.2629e-02],\n",
      "          [ 2.2495e-02,  3.5717e-02, -8.1405e-02, -3.5083e-02, -1.1438e-01]],\n",
      "\n",
      "         [[-3.8139e-02,  4.7913e-02,  3.7187e-02, -7.0583e-02, -4.9874e-02],\n",
      "          [-1.9412e-02,  3.7945e-02,  7.8863e-02, -2.7446e-02, -1.0813e-01],\n",
      "          [-2.4044e-02, -5.8333e-02,  9.9970e-02,  6.1608e-02, -9.3001e-02],\n",
      "          [ 3.1813e-02,  1.5604e-01,  2.8571e-02, -1.3250e-02, -7.6817e-02],\n",
      "          [ 1.9598e-03,  6.0216e-02, -5.6361e-02,  1.9059e-02, -9.7259e-03]],\n",
      "\n",
      "         [[ 1.7288e-02, -6.2159e-02,  2.2005e-02, -2.1023e-02, -5.3290e-02],\n",
      "          [ 3.6678e-03, -9.8477e-02, -4.8801e-02, -7.6244e-02, -1.1241e-01],\n",
      "          [ 5.7338e-02, -1.7582e-03,  4.3068e-02,  3.6934e-02, -1.3315e-02],\n",
      "          [ 4.7197e-02,  5.2916e-02,  3.9103e-02,  5.8215e-02, -8.1552e-02],\n",
      "          [-1.5406e-02,  1.0859e-01,  1.2390e-01,  4.6360e-02, -1.4763e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.9875e-02, -2.4023e-02, -1.6072e-01, -9.4910e-02, -1.4943e-01],\n",
      "          [ 7.0709e-02, -1.4761e-01, -9.6688e-02, -1.4313e-01, -7.7272e-02],\n",
      "          [ 6.1041e-03,  1.2151e-02, -8.2109e-03, -2.0195e-01,  1.4982e-02],\n",
      "          [ 1.3706e-01,  1.1543e-01,  3.1755e-03, -9.0505e-02,  4.6196e-02],\n",
      "          [ 6.7890e-02,  1.2411e-01,  7.6374e-02,  8.5388e-02,  1.2827e-01]],\n",
      "\n",
      "         [[ 4.4989e-03,  2.0330e-02, -8.9829e-02, -1.0901e-01, -8.3900e-02],\n",
      "          [ 5.6715e-02,  4.4719e-02,  7.1435e-03, -9.9429e-02, -3.9316e-02],\n",
      "          [ 5.5210e-02,  7.3781e-03,  1.2632e-02, -4.9596e-02, -5.9201e-02],\n",
      "          [-4.4648e-02,  2.1512e-02, -4.4091e-02, -4.2192e-02,  2.6324e-02],\n",
      "          [-2.4327e-02, -5.2195e-02, -9.4592e-03,  1.7752e-02,  1.4266e-02]],\n",
      "\n",
      "         [[-4.2721e-02, -5.6880e-03,  4.5651e-02,  4.0760e-03, -4.1621e-02],\n",
      "          [-3.4990e-02, -5.5595e-02, -6.6747e-02,  1.6797e-02, -1.4385e-03],\n",
      "          [ 3.6952e-03,  5.6515e-02, -1.7536e-02,  3.5418e-02, -3.8939e-02],\n",
      "          [ 7.7012e-02, -2.7888e-02,  8.4735e-03,  6.6786e-02,  2.2785e-02],\n",
      "          [ 5.7473e-02,  4.6615e-02,  5.4519e-02,  1.3857e-02,  3.3108e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.2550e-02, -4.1085e-02, -2.2761e-02, -2.8879e-02, -4.8093e-02],\n",
      "          [ 4.1729e-02,  3.4346e-02,  2.6311e-02, -3.0999e-02, -3.8758e-02],\n",
      "          [ 1.3166e-02,  3.1349e-02,  7.6738e-02, -7.9117e-02, -4.4334e-02],\n",
      "          [ 3.8693e-02,  2.4617e-02, -2.1188e-02, -6.4069e-02, -4.0784e-02],\n",
      "          [ 1.7838e-02, -8.2012e-03, -1.7051e-02, -5.0119e-02, -7.5968e-02]],\n",
      "\n",
      "         [[ 3.7057e-02, -3.5076e-02, -3.1369e-02, -1.0654e-01, -1.4330e-01],\n",
      "          [-2.9657e-02,  5.7143e-03, -8.1811e-02, -4.1453e-02, -4.9050e-02],\n",
      "          [ 5.9370e-02,  9.7631e-03, -1.3049e-02, -8.3036e-02, -9.0627e-02],\n",
      "          [ 5.8401e-02,  2.4747e-02, -1.4045e-03, -9.5652e-02, -2.9605e-02],\n",
      "          [ 7.5555e-02,  6.1759e-02,  7.9413e-02, -3.0572e-03,  8.4210e-02]],\n",
      "\n",
      "         [[-1.3082e-02,  7.7323e-03, -1.9489e-02, -3.9600e-02, -1.8921e-02],\n",
      "          [ 5.1465e-02, -2.4362e-02, -1.0136e-01, -1.0072e-03, -6.0763e-02],\n",
      "          [-3.8113e-02, -9.6998e-02, -5.8158e-02, -1.2301e-01, -1.4512e-01],\n",
      "          [ 9.4812e-02,  3.8605e-02, -1.2168e-01, -9.4010e-02, -2.4485e-02],\n",
      "          [-8.8958e-03, -1.2726e-02,  7.5058e-03, -7.4821e-02, -5.1017e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.7253e-02,  7.1413e-02,  1.2242e-01,  1.1253e-01, -7.0330e-02],\n",
      "          [-1.5676e-01, -1.4993e-01, -1.5537e-01, -7.0927e-02, -2.2511e-02],\n",
      "          [-6.7388e-02, -1.6395e-01, -1.5600e-01, -1.2513e-01, -4.4132e-02],\n",
      "          [ 6.1223e-02, -7.0085e-02, -1.2884e-01, -7.7443e-02,  3.1493e-02],\n",
      "          [ 9.8219e-02,  8.3832e-02,  3.2432e-02, -8.8248e-02,  6.6165e-02]],\n",
      "\n",
      "         [[ 2.4673e-02,  8.9109e-02,  7.4293e-02,  2.6819e-02, -2.7140e-02],\n",
      "          [ 3.6028e-02, -1.0117e-01, -1.7506e-02, -2.9346e-02,  3.0547e-02],\n",
      "          [ 6.0968e-02,  2.9355e-02, -4.2524e-02, -2.9464e-03,  1.1897e-02],\n",
      "          [ 5.9213e-02,  4.9999e-03,  3.6968e-02, -4.3065e-02,  4.8208e-02],\n",
      "          [-3.8051e-02,  6.9815e-03,  4.1922e-02,  4.9591e-03,  2.0320e-02]],\n",
      "\n",
      "         [[-6.7087e-02,  2.4142e-02,  5.8175e-02, -4.4753e-02,  4.0077e-02],\n",
      "          [ 2.3520e-02, -2.5875e-02,  1.8374e-02, -5.4048e-03, -5.8325e-02],\n",
      "          [-4.6690e-02,  1.5977e-02,  5.0004e-02,  7.8570e-03,  2.6852e-02],\n",
      "          [ 4.4385e-02, -1.3769e-03, -3.2854e-02,  1.9912e-02, -5.4965e-02],\n",
      "          [ 7.3508e-02,  8.7638e-02, -5.0948e-02, -5.6012e-02, -1.0465e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.5617e-02,  1.2618e-02,  2.6661e-02, -3.8708e-02, -3.0176e-02],\n",
      "          [ 2.9155e-02, -3.5322e-04,  3.0270e-02,  3.1103e-02, -2.6821e-02],\n",
      "          [ 8.7295e-02, -3.0971e-03, -8.0530e-02, -1.5309e-02,  7.0959e-02],\n",
      "          [ 8.0555e-02,  9.1442e-02, -2.9447e-02, -3.1224e-02, -1.9233e-02],\n",
      "          [ 3.6764e-02, -1.4728e-03, -3.4177e-02,  3.9005e-02,  6.1956e-02]],\n",
      "\n",
      "         [[ 4.4204e-02,  4.5235e-02,  6.0426e-02, -6.9728e-02, -7.3531e-02],\n",
      "          [ 7.6474e-02, -4.5638e-02, -1.5466e-01, -1.2145e-01, -1.6696e-02],\n",
      "          [ 4.4971e-02,  4.6784e-02, -2.8111e-02, -7.5477e-02, -6.0446e-02],\n",
      "          [ 6.1286e-02,  9.6701e-03, -3.1174e-02, -9.7805e-02,  2.1777e-02],\n",
      "          [ 1.0286e-01,  1.4678e-02, -6.6829e-02, -4.2836e-02, -3.5486e-03]],\n",
      "\n",
      "         [[ 1.2011e-01,  1.2396e-01,  9.3463e-02, -3.4288e-02, -1.5115e-02],\n",
      "          [ 3.3953e-02, -7.5995e-02, -1.0519e-01, -2.7984e-02, -9.2412e-02],\n",
      "          [ 8.8554e-02, -1.7335e-02, -1.0722e-01, -1.6412e-01, -4.4703e-02],\n",
      "          [-1.7729e-02, -3.2085e-02, -7.5602e-02, -5.1624e-02, -5.5660e-02],\n",
      "          [ 3.5002e-03,  4.2478e-02, -5.8035e-02, -7.7437e-02, -1.1758e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.1992e-02, -3.9646e-03, -2.8970e-02, -6.2418e-02, -2.6261e-03],\n",
      "          [-4.1107e-02, -3.6460e-02, -3.6214e-02, -1.3471e-01, -1.2420e-01],\n",
      "          [ 2.5267e-02,  4.0044e-03, -4.2144e-03, -9.2199e-02, -1.1348e-01],\n",
      "          [ 6.5076e-02, -4.3164e-02, -2.1705e-02,  6.9143e-02,  3.0734e-02],\n",
      "          [ 3.9408e-02,  7.5789e-02,  1.6097e-01,  1.4404e-01,  5.2713e-02]],\n",
      "\n",
      "         [[-8.1354e-02, -7.6676e-02,  1.0501e-02, -5.3904e-02,  4.2758e-02],\n",
      "          [-5.1817e-02,  7.5073e-02,  1.7291e-02,  8.1365e-03,  3.6917e-02],\n",
      "          [ 2.3695e-02,  6.6455e-02,  7.2109e-03,  2.0252e-02, -4.8223e-02],\n",
      "          [-6.2374e-03,  1.7337e-02,  7.1223e-02,  3.7131e-02,  4.0289e-02],\n",
      "          [ 2.8529e-02,  1.0357e-02,  9.1669e-03,  2.3038e-02, -3.7254e-02]],\n",
      "\n",
      "         [[-3.8394e-03, -2.7920e-02, -1.1941e-02,  6.2096e-02,  7.3860e-02],\n",
      "          [-5.3652e-02, -2.5930e-03, -2.4075e-02, -4.6174e-02,  8.9186e-03],\n",
      "          [ 2.4286e-02, -6.1407e-02, -1.2048e-01, -9.9933e-03,  3.3164e-02],\n",
      "          [ 2.2012e-02, -2.5517e-02,  4.1995e-02,  8.1476e-02,  7.8758e-02],\n",
      "          [ 2.7978e-02,  8.1599e-02,  7.7226e-02,  1.0331e-02,  3.0336e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2701e-02, -1.2527e-02,  1.7301e-03,  8.0618e-02,  5.1430e-02],\n",
      "          [-6.9872e-02,  5.8992e-02,  1.3094e-01,  6.1457e-02,  1.8204e-03],\n",
      "          [-7.7030e-02, -7.0076e-03,  7.9267e-02,  6.8633e-02,  2.0831e-02],\n",
      "          [ 6.7627e-02, -9.9290e-03,  2.9548e-02,  5.3354e-02,  5.9734e-02],\n",
      "          [ 5.8702e-02,  5.6177e-02,  6.0717e-03, -3.6555e-03, -1.3968e-02]],\n",
      "\n",
      "         [[-1.1022e-01, -2.5046e-02, -2.5357e-02, -7.5223e-02, -1.1108e-01],\n",
      "          [ 6.2067e-02,  7.9381e-02, -3.7713e-02,  1.8544e-02, -5.0996e-02],\n",
      "          [ 8.9246e-02,  4.0109e-02, -4.5316e-03, -6.3708e-02, -4.5045e-02],\n",
      "          [-2.3346e-02,  8.3914e-02,  7.3497e-02,  3.5563e-02,  1.8540e-02],\n",
      "          [ 3.6366e-02,  8.7535e-02,  5.1613e-03,  5.6160e-02, -1.2465e-01]],\n",
      "\n",
      "         [[-5.8593e-02,  5.0377e-03,  1.6600e-02,  1.8031e-02, -6.1455e-02],\n",
      "          [-1.9913e-03, -7.0206e-03, -2.4748e-02,  4.8176e-03, -8.1032e-02],\n",
      "          [ 8.8310e-02, -1.5406e-02, -2.9585e-02, -4.9254e-02,  1.8120e-03],\n",
      "          [ 3.6073e-03, -1.6437e-03,  5.6675e-02, -4.4636e-02, -4.1292e-02],\n",
      "          [ 1.0682e-02,  1.0946e-01,  7.4015e-03,  3.6422e-02, -1.1038e-02]]]])\n",
      "torch.Size([20, 10, 5, 5])\n",
      "tensor([-0.0449, -0.0158, -0.0449,  0.0517, -0.0378, -0.0210,  0.0481, -0.0192,\n",
      "         0.0016,  0.0179,  0.0494, -0.0005, -0.0095, -0.0442, -0.0263,  0.0405,\n",
      "        -0.0723, -0.0242,  0.0596, -0.0329])\n",
      "torch.Size([20])\n",
      "tensor([[ 0.0523, -0.0234,  0.0580,  ...,  0.0271, -0.0242, -0.0190],\n",
      "        [-0.0055,  0.0674,  0.0323,  ...,  0.0360, -0.0289, -0.0607],\n",
      "        [ 0.0126,  0.0260,  0.0221,  ...,  0.0513,  0.0303,  0.0090],\n",
      "        ...,\n",
      "        [-0.0128, -0.0360, -0.0200,  ...,  0.0571,  0.0134,  0.0083],\n",
      "        [-0.0459, -0.0019,  0.0228,  ...,  0.0314, -0.0004,  0.0443],\n",
      "        [ 0.0276,  0.0375,  0.0195,  ..., -0.0399, -0.0620,  0.0129]])\n",
      "torch.Size([50, 320])\n",
      "tensor([ 0.0655,  0.0268,  0.0440, -0.0392,  0.0683,  0.0429,  0.1238, -0.0229,\n",
      "         0.0418,  0.0346, -0.0236,  0.0526,  0.0139, -0.0177,  0.0294,  0.0635,\n",
      "        -0.0273,  0.0843,  0.0024,  0.0657, -0.0065, -0.0107,  0.1001,  0.0807,\n",
      "         0.0171,  0.0400,  0.0805,  0.0404,  0.0768,  0.0136,  0.0586,  0.1002,\n",
      "         0.0096,  0.0228,  0.0104, -0.0516, -0.0042,  0.0501, -0.0263,  0.0652,\n",
      "         0.0262, -0.0149,  0.1015, -0.0120, -0.0500,  0.0580,  0.0403, -0.0262,\n",
      "        -0.0143, -0.0009])\n",
      "torch.Size([50])\n",
      "tensor([[ 1.0943e-02,  2.1897e-01,  1.0986e-01,  2.3701e-01, -5.2931e-02,\n",
      "          1.4537e-01,  9.4044e-03,  6.5003e-02, -1.2785e-01, -1.7417e-01,\n",
      "          1.3605e-01, -1.6474e-01, -1.3353e-01,  1.8083e-01,  1.9020e-02,\n",
      "          1.8013e-02,  3.8902e-01, -1.3456e-01, -1.5921e-01, -2.6778e-01,\n",
      "         -5.6452e-02, -4.9562e-02,  1.7704e-01, -8.6141e-02,  1.9224e-01,\n",
      "         -2.3605e-01,  2.7440e-01, -1.2859e-01, -1.2056e-01, -1.3720e-01,\n",
      "          2.5307e-01, -2.2218e-01, -1.0321e-01, -1.7019e-01, -8.5504e-02,\n",
      "         -5.0978e-02,  1.8338e-01,  1.6687e-01, -2.8411e-01,  2.8616e-01,\n",
      "         -1.4714e-01, -1.4034e-01, -4.5521e-02, -5.1168e-02,  1.6007e-01,\n",
      "          1.4651e-01, -2.7762e-01,  1.8891e-01, -1.0761e-01, -2.9974e-02],\n",
      "        [ 2.3649e-01,  1.5220e-01, -2.7021e-01, -1.0963e-01,  2.9309e-02,\n",
      "         -3.0358e-01,  4.7376e-01, -2.1196e-01,  2.0240e-01,  2.0550e-01,\n",
      "         -1.5885e-01,  3.3752e-01, -2.4247e-01, -1.8011e-01, -1.6099e-01,\n",
      "         -2.0531e-01, -9.9568e-02,  2.3587e-01, -8.1980e-03,  2.4268e-01,\n",
      "         -8.5307e-02, -2.4724e-02,  2.5447e-01,  2.1056e-01, -3.1327e-01,\n",
      "          2.1170e-01, -5.4418e-02, -1.3324e-01,  1.7670e-01,  1.2349e-01,\n",
      "         -1.4906e-01,  1.3861e-01, -1.9406e-01, -2.2438e-01,  3.3968e-01,\n",
      "         -9.6438e-03, -1.8679e-01, -2.7123e-01, -1.9373e-01, -8.9320e-02,\n",
      "         -2.2351e-01,  1.2391e-01,  2.0596e-01,  6.5059e-02, -1.3447e-01,\n",
      "          2.1433e-01,  2.1235e-01, -5.7837e-02, -7.9451e-02, -7.7333e-02],\n",
      "        [ 2.0424e-01, -2.0111e-01, -1.5704e-01,  1.9408e-01, -1.8076e-01,\n",
      "         -2.0873e-01, -1.1149e-01,  5.5887e-02, -9.9471e-02,  1.3896e-01,\n",
      "          1.2844e-01,  2.8468e-02, -1.0447e-01, -2.1309e-01, -1.3232e-01,\n",
      "         -5.8846e-02, -9.4912e-02,  2.0543e-01, -1.4133e-01, -2.2987e-01,\n",
      "         -1.0846e-01,  1.8229e-01, -1.6595e-01,  1.9819e-01, -6.2734e-02,\n",
      "         -1.2025e-01,  1.2723e-01,  1.2805e-01,  2.3215e-01, -2.5134e-01,\n",
      "          2.3848e-01, -1.7074e-01,  6.0511e-02, -1.0112e-01,  3.3378e-01,\n",
      "         -7.2705e-02,  1.5967e-01, -6.3630e-02, -1.6985e-01, -8.5164e-02,\n",
      "          8.0810e-02,  1.8649e-01,  2.0456e-01,  1.1396e-01,  1.8033e-01,\n",
      "         -1.5391e-01,  2.3131e-01,  1.8169e-01,  9.7340e-02,  2.9438e-01],\n",
      "        [ 2.0710e-01, -1.7945e-01, -2.4604e-01, -1.1386e-02, -1.4498e-01,\n",
      "          1.8986e-01, -5.5808e-02,  1.8920e-01,  2.0329e-01,  2.3266e-01,\n",
      "         -1.8246e-01, -1.3115e-01, -4.3837e-02, -1.5295e-01, -6.0538e-02,\n",
      "         -1.3138e-01, -7.8296e-02,  1.1455e-03,  1.7963e-01, -1.6175e-01,\n",
      "         -4.5201e-02,  1.5524e-01, -1.7405e-01, -1.9292e-01,  1.9769e-01,\n",
      "         -4.9889e-02, -2.0310e-01, -1.2433e-01,  2.0802e-01,  2.0682e-01,\n",
      "         -3.6147e-02,  5.4204e-02, -1.6826e-01,  1.9319e-01, -1.4958e-01,\n",
      "          1.3639e-01, -1.5176e-01, -7.0069e-02, -7.8489e-02,  1.5794e-01,\n",
      "          1.9550e-01,  2.1618e-01, -1.9044e-01,  1.1940e-01,  1.7226e-01,\n",
      "         -1.9718e-01, -8.6491e-02,  1.6267e-01,  1.3144e-01, -2.8169e-02],\n",
      "        [ 2.5342e-01, -1.8319e-01, -2.2287e-01, -6.0202e-02, -1.3760e-01,\n",
      "         -2.5015e-01,  1.4071e-01, -1.7763e-01, -1.4469e-01, -4.9634e-02,\n",
      "          1.4966e-01, -1.8417e-02,  1.9152e-01,  2.5116e-01,  2.8024e-01,\n",
      "         -1.5660e-01, -2.6205e-02, -5.9980e-02,  1.0766e-01,  2.8337e-01,\n",
      "          2.7778e-01, -1.3121e-01,  7.6380e-02,  2.5841e-01, -1.3795e-01,\n",
      "          6.9703e-02, -1.4481e-01,  2.1856e-01, -1.1844e-01, -1.7726e-01,\n",
      "         -1.5058e-01, -1.1957e-01,  3.4777e-01,  1.8675e-01, -1.1073e-01,\n",
      "          9.7263e-02,  1.2151e-01, -2.5114e-01,  1.2947e-01, -6.4250e-02,\n",
      "          1.7149e-01, -9.5006e-02, -6.5949e-02, -2.5718e-01, -6.0149e-03,\n",
      "          2.3484e-01,  2.3427e-01, -2.0695e-01, -9.2094e-02,  3.0302e-01],\n",
      "        [-5.8684e-02,  2.1499e-01,  1.2283e-01, -2.0328e-01,  2.7118e-01,\n",
      "          1.8896e-01, -9.3529e-02,  1.8241e-01, -2.1543e-01, -6.7627e-02,\n",
      "         -1.0407e-01, -2.1570e-01,  2.1540e-01, -2.9444e-02, -1.0151e-01,\n",
      "         -1.8735e-01, -1.3644e-01, -1.5065e-01,  1.7660e-01, -1.7821e-01,\n",
      "         -1.8628e-01,  1.3129e-02,  1.6872e-01, -1.4536e-01,  9.7382e-02,\n",
      "          2.3789e-01, -1.5883e-01, -2.4178e-01, -2.3134e-01,  1.9000e-01,\n",
      "         -6.0886e-02,  1.8471e-01, -4.5411e-02,  1.8329e-01, -4.8253e-02,\n",
      "          1.6480e-01, -1.1735e-01,  1.5832e-01, -6.3882e-02,  3.7841e-03,\n",
      "          1.6182e-01, -8.3848e-02, -1.8124e-02, -2.9123e-01,  6.2432e-02,\n",
      "          1.9150e-01, -1.4061e-01, -1.8209e-01,  1.2620e-01, -4.9654e-02],\n",
      "        [ 6.4442e-02,  2.0729e-01,  1.0917e-01,  1.2728e-01,  2.6513e-01,\n",
      "          1.6202e-01, -1.2435e-01,  1.4636e-01, -1.4936e-01, -2.9667e-01,\n",
      "          1.2145e-01,  3.1890e-01,  1.8653e-01,  2.2770e-01,  2.8368e-01,\n",
      "         -4.7254e-02, -1.2430e-01,  7.6512e-02, -3.4584e-01, -1.4214e-01,\n",
      "         -2.5846e-01, -9.2421e-02, -3.3524e-02,  2.3540e-01, -2.7605e-01,\n",
      "         -1.8564e-01, -1.3543e-01, -1.8681e-01, -1.8605e-01,  1.6893e-01,\n",
      "         -1.3073e-01,  1.5284e-01,  3.5027e-01, -2.8878e-01, -4.5872e-02,\n",
      "         -2.5505e-01,  1.5855e-01, -2.3709e-02, -3.2544e-01,  2.5645e-01,\n",
      "         -3.4467e-01, -2.8956e-01,  2.1673e-01, -4.2980e-01,  1.5318e-01,\n",
      "          1.0770e-01,  1.1648e-02, -2.1606e-01,  7.4427e-02, -7.8106e-02],\n",
      "        [ 1.4024e-02,  1.2599e-01,  5.4156e-02, -7.6334e-02, -1.5877e-01,\n",
      "         -2.4391e-01, -1.0230e-02, -3.4939e-01, -8.9395e-02,  2.0765e-01,\n",
      "         -2.3936e-01, -5.0726e-02, -2.1437e-01, -9.1257e-02,  4.9604e-02,\n",
      "          2.8119e-01,  5.6647e-02,  2.1566e-01,  1.7167e-01, -1.0375e-02,\n",
      "          3.0738e-01,  2.1126e-01,  2.2186e-01, -7.4943e-02, -7.8163e-02,\n",
      "         -1.3071e-01,  1.7959e-01,  2.3403e-01,  2.0698e-01, -1.5802e-01,\n",
      "          2.1361e-01, -2.6845e-01, -1.6481e-01, -1.1154e-01, -6.7645e-02,\n",
      "          1.3629e-01, -2.2385e-01,  1.5516e-01,  1.3346e-01, -1.7254e-01,\n",
      "          1.6459e-01,  2.1002e-01, -1.1409e-01,  9.4774e-02, -2.3030e-01,\n",
      "         -1.7123e-01,  2.3313e-01,  2.1383e-03, -1.2003e-01, -1.1268e-01],\n",
      "        [-1.4286e-01,  1.4025e-01,  8.7197e-02, -6.0979e-02,  9.0102e-03,\n",
      "          7.7399e-02, -1.5539e-01,  1.9134e-01,  2.0425e-01, -9.5950e-02,\n",
      "         -1.1935e-01, -1.2835e-01,  2.2827e-01,  1.7302e-01, -9.6168e-02,\n",
      "         -5.9972e-02, -9.4965e-02, -1.7135e-01, -8.7994e-02,  1.6674e-01,\n",
      "         -7.8618e-02, -5.4987e-02, -1.0343e-01, -5.6217e-02, -2.5973e-05,\n",
      "          3.8902e-02, -1.1179e-01, -4.0602e-02,  2.2305e-01,  1.3499e-01,\n",
      "         -6.7089e-02,  1.6669e-01, -6.1206e-02,  1.7743e-01, -7.5882e-03,\n",
      "         -2.1990e-01, -3.6171e-02,  1.6494e-01,  1.5234e-01, -1.2450e-01,\n",
      "         -1.1589e-01, -7.7503e-02,  2.3086e-01,  9.6946e-02,  1.9028e-01,\n",
      "         -8.3923e-02,  2.2019e-01,  1.9582e-01,  2.5793e-02, -9.1085e-02],\n",
      "        [-1.9812e-01,  1.0491e-01, -1.5598e-01,  4.8536e-02, -1.6992e-01,\n",
      "         -7.6884e-02, -6.5281e-02,  1.3287e-01,  8.7982e-02, -1.3784e-01,\n",
      "          1.0939e-01, -8.9065e-02,  2.1865e-01,  2.2783e-01, -2.3471e-01,\n",
      "          2.6598e-01, -1.2510e-01, -2.3837e-01,  1.8651e-01,  2.3334e-01,\n",
      "          2.9030e-01,  1.7949e-01, -1.3336e-02, -1.6794e-01,  2.2369e-01,\n",
      "          2.4574e-01, -8.9074e-02,  2.2010e-01, -2.4063e-02, -4.5820e-02,\n",
      "         -1.4096e-01, -2.0313e-01, -1.0912e-01,  2.2013e-01, -2.0165e-01,\n",
      "          1.1451e-01,  1.4490e-01, -9.5904e-02,  1.4262e-01, -1.0932e-01,\n",
      "          1.6954e-01, -1.5172e-01, -1.9919e-01,  8.5753e-02, -9.0030e-02,\n",
      "         -1.4152e-01, -1.0617e-01, -2.6286e-02, -1.9187e-01, -1.6863e-01]])\n",
      "torch.Size([10, 50])\n",
      "tensor([ 0.0070,  0.4794, -0.1613, -0.1276, -0.0939, -0.1648, -0.2415, -0.1022,\n",
      "         0.2112,  0.2106])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in model_torch.parameters():\n",
    "    print(param.data)\n",
    "    print(param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "imported = tf.saved_model.load('mnist.pb')\n",
    "f = imported.signatures[\"serving_default\"]\n",
    "#print(f(x=tf.constant([[1.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       " array([[-1674.5398 , -1686.7817 ,  -955.3423 ,     0.     , -1886.2817 ,\n",
       "         -1205.1274 , -2681.6333 , -1189.6688 ,  -993.9929 ,  -944.36743]],\n",
       "       dtype=float32)>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(tf.convert_to_tensor(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "model_new = tf.keras.models.load_model('mnist.pb')\n",
    "f_new = model_new.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 10)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 10)\n",
    "        self.fc1 = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(self.fc(x)))\n",
    "        return x\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fc\n",
      "fc1\n",
      "relu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2324]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "# out of place\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "d = dict(model.named_modules())\n",
    "\n",
    "for i in d:\n",
    "    print(i)\n",
    "    model.fc1.register_forward_hook(get_activation(i))\n",
    "\n",
    "model.fc.register_forward_hook(get_activation('fc'))\n",
    "output = model(x)\n",
    "activation['fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[0.2324]])\n",
      "fc\n",
      "tensor([[ 0.3466,  0.5461,  0.1191, -0.3720, -0.6417,  0.6475,  0.3437, -0.1969,\n",
      "         -0.3114, -1.3142]])\n",
      "fc1\n",
      "tensor([[0.2324]])\n",
      "relu\n",
      "tensor([[0.2324]])\n",
      "<generator object Module.named_modules at 0x7f5b2b3ccc80>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "hooks = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        hooks[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n",
    "    hooks[name] = module.register_forward_hook(get_activation(name))\n",
    "    output = model(x)\n",
    "    print(hooks[name])\n",
    "lists = model.named_modules()\n",
    "print(lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(x)\n",
    "print(hooks['fc'])\n",
    "output = model(x)\n",
    "print(hooks['fc1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(1, 28, 28).to(device)\n",
    "model = Net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[-2.1330, -2.4107, -2.3183, -2.2274, -2.2931, -2.4593, -2.1553, -2.4574,\n",
      "         -2.3949, -2.2410]], device='cuda:0')\n",
      "conv1\n",
      "tensor([[[ 0.3552, -0.3835, -0.3798,  ...,  0.6531, -0.3819,  0.9250],\n",
      "         [-0.3931,  0.4347, -0.8004,  ...,  1.0041,  0.2548,  0.4114],\n",
      "         [-0.1936, -0.5977,  0.3851,  ..., -1.1718, -0.1572,  0.4118],\n",
      "         ...,\n",
      "         [-0.4834,  0.4055,  0.8806,  ..., -0.4012, -0.4326, -0.0167],\n",
      "         [-0.1985, -1.4211, -0.9980,  ...,  0.7819,  0.6955, -0.2902],\n",
      "         [-0.3093, -0.8619, -0.4449,  ...,  0.1748,  0.0424, -1.1426]],\n",
      "\n",
      "        [[-0.0847,  0.2656,  0.2576,  ..., -0.0345,  0.3506,  0.3423],\n",
      "         [ 1.1895,  0.7303,  0.1480,  ...,  0.1669, -0.4908, -0.0397],\n",
      "         [-1.1355,  0.1719, -0.3975,  ...,  0.9463,  0.8590,  1.0641],\n",
      "         ...,\n",
      "         [ 0.7148,  1.6135,  0.8700,  ..., -0.7308,  0.3505, -1.2597],\n",
      "         [-0.3741,  0.4877,  0.0056,  ...,  0.5539, -0.4294,  0.9344],\n",
      "         [ 0.6951, -0.0517,  0.2878,  ...,  0.4206, -0.5365, -0.2273]],\n",
      "\n",
      "        [[-0.3965, -0.0846, -0.6770,  ...,  0.3826, -0.1961,  0.9403],\n",
      "         [-0.3917, -0.3442, -0.1947,  ..., -0.5598,  0.0302,  0.7406],\n",
      "         [ 0.6113,  0.3902, -0.1299,  ...,  0.3587,  0.0834, -0.1952],\n",
      "         ...,\n",
      "         [ 0.6382,  1.0207, -0.4104,  ..., -0.2449,  0.0860,  0.3255],\n",
      "         [-0.5910, -0.5283,  0.0113,  ..., -0.1479, -0.0193, -0.6142],\n",
      "         [ 0.7011, -0.2371,  0.0057,  ...,  0.3454, -0.2333,  0.0970]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2080, -0.2034,  0.2297,  ..., -0.0387, -0.6917, -1.2001],\n",
      "         [ 1.6867,  1.0332,  0.3343,  ..., -0.4704, -0.4088,  0.2113],\n",
      "         [ 0.1154,  0.3841, -0.2912,  ..., -0.1660,  1.1243,  0.3549],\n",
      "         ...,\n",
      "         [ 0.3401,  0.7442,  0.6595,  ..., -0.0873,  0.9203,  0.2291],\n",
      "         [-0.3741,  0.0197,  0.9182,  ...,  0.2116,  0.3310,  0.2400],\n",
      "         [ 0.2883,  0.7833,  1.0261,  ...,  0.6809, -0.4337,  0.0369]],\n",
      "\n",
      "        [[-0.1825,  0.3494, -0.5677,  ..., -0.0633, -0.8800, -1.1053],\n",
      "         [ 0.3739,  0.0716,  0.4765,  ..., -0.2875,  0.0505, -0.3824],\n",
      "         [-0.0361,  0.0807,  0.0830,  ...,  0.1240,  0.6304, -0.8145],\n",
      "         ...,\n",
      "         [-0.3692, -0.5942, -0.2331,  ..., -0.0827,  0.5695,  0.1136],\n",
      "         [-0.1105, -0.9267,  0.3904,  ...,  0.4008,  0.8944, -0.3496],\n",
      "         [-0.8175, -0.4259,  0.7078,  ...,  0.7821,  0.1536,  0.7034]],\n",
      "\n",
      "        [[ 0.3449, -0.0631,  0.5519,  ...,  0.8767, -0.0966, -0.1899],\n",
      "         [-0.2241, -0.3832, -0.2326,  ..., -0.0276, -0.1158,  0.4633],\n",
      "         [-0.1402,  0.6182, -0.2511,  ..., -0.0932, -0.1777,  0.1527],\n",
      "         ...,\n",
      "         [ 0.1971, -0.0018, -0.1508,  ..., -0.7596,  0.2823, -0.0871],\n",
      "         [ 0.1237,  0.1230, -0.3579,  ..., -0.5728,  0.0435,  0.3259],\n",
      "         [-0.3618, -0.2624, -1.4031,  ...,  0.2457,  0.2963,  0.8327]]],\n",
      "       device='cuda:0')\n",
      "conv2\n",
      "tensor([[[ 0.1897,  0.2000, -0.0923,  ...,  0.0455,  0.2577,  0.1029],\n",
      "         [ 0.4680, -0.1014,  0.3526,  ...,  0.4331,  0.1471,  0.5662],\n",
      "         [ 0.2786,  0.6706,  0.2965,  ...,  0.2715, -0.0325,  0.3053],\n",
      "         ...,\n",
      "         [-0.0417, -0.0302,  0.1755,  ...,  0.2680, -0.0229,  0.4409],\n",
      "         [ 0.0370,  0.1214,  0.5230,  ...,  0.3898,  0.1992,  0.1753],\n",
      "         [-0.0026,  0.2461,  0.2052,  ...,  0.1002,  0.5201,  0.4357]],\n",
      "\n",
      "        [[-0.1707, -0.2158, -0.5382,  ..., -0.2165, -0.4427, -0.6166],\n",
      "         [-0.8529, -0.7117, -0.2956,  ..., -0.5081, -0.2280, -0.6186],\n",
      "         [-0.3536, -0.5622, -0.2257,  ..., -0.5816, -0.4873, -0.0882],\n",
      "         ...,\n",
      "         [-0.3236, -0.5790, -0.1929,  ..., -0.4649, -0.5043, -0.6842],\n",
      "         [-0.3003, -0.4034, -0.4859,  ..., -0.5080, -0.5723, -0.3810],\n",
      "         [-0.5071, -0.6743, -0.6852,  ..., -0.5152, -0.1174, -0.6006]],\n",
      "\n",
      "        [[ 0.3080,  0.7471,  0.1995,  ...,  0.2001,  0.5979,  0.3039],\n",
      "         [ 0.4852,  0.5978,  0.4319,  ...,  0.4833,  0.1399,  0.5260],\n",
      "         [ 0.4826,  0.4843,  0.5471,  ...,  0.3068,  0.2997,  0.3518],\n",
      "         ...,\n",
      "         [ 0.1336,  0.3448,  0.2716,  ...,  0.5353,  0.3587,  0.2544],\n",
      "         [ 0.1752,  0.5470,  0.3395,  ...,  0.2586,  0.2970,  0.5606],\n",
      "         [ 0.0993,  0.5733,  0.7139,  ...,  0.8857,  0.3025,  0.1215]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3378,  0.8123,  0.6327,  ...,  0.5158,  0.5590,  0.4201],\n",
      "         [ 0.4262,  0.5598,  0.7421,  ...,  0.6060,  0.5343,  0.4358],\n",
      "         [ 0.6568,  0.3600,  0.5380,  ...,  1.1967,  0.9071,  0.8156],\n",
      "         ...,\n",
      "         [ 0.3227,  0.7769,  0.0418,  ...,  0.2247,  0.9435,  0.5999],\n",
      "         [ 0.4431,  0.6709,  0.8159,  ...,  0.7661,  0.5470,  0.4448],\n",
      "         [ 0.8717,  0.7225,  0.5593,  ...,  0.7157,  0.7661,  0.5997]],\n",
      "\n",
      "        [[-0.8892, -0.4422, -0.5848,  ..., -0.7318, -0.2747, -0.5929],\n",
      "         [-0.2594, -0.5827, -0.2503,  ..., -0.2535, -0.1947, -0.1857],\n",
      "         [-0.8834, -0.1026, -0.5914,  ...,  0.0406, -0.4827, -0.0935],\n",
      "         ...,\n",
      "         [-0.3792, -0.3211, -0.4488,  ...,  0.0448, -0.3395, -0.1040],\n",
      "         [-0.1986, -0.8214, -0.5787,  ..., -0.3729, -0.1375, -0.3797],\n",
      "         [-0.7646, -0.3454,  0.0702,  ..., -0.6666, -0.4603, -0.1234]],\n",
      "\n",
      "        [[-0.7924, -0.9549, -0.6127,  ..., -0.4825, -0.7709, -0.7548],\n",
      "         [-0.4939, -0.5083, -0.9074,  ..., -0.8261, -0.2936, -0.4845],\n",
      "         [-0.7622, -0.9882, -0.6497,  ..., -0.8379, -0.6117, -0.5560],\n",
      "         ...,\n",
      "         [-0.4593, -0.4404, -0.5228,  ..., -0.5077, -0.7412, -0.8846],\n",
      "         [-0.9737, -0.7799, -0.9781,  ..., -0.2598, -1.0383, -0.6741],\n",
      "         [-0.3467, -0.5346, -0.9050,  ..., -0.8086, -0.3045, -0.4631]]],\n",
      "       device='cuda:0')\n",
      "conv2_drop\n",
      "tensor([[[ 0.1897,  0.2000, -0.0923,  ...,  0.0455,  0.2577,  0.1029],\n",
      "         [ 0.4680, -0.1014,  0.3526,  ...,  0.4331,  0.1471,  0.5662],\n",
      "         [ 0.2786,  0.6706,  0.2965,  ...,  0.2715, -0.0325,  0.3053],\n",
      "         ...,\n",
      "         [-0.0417, -0.0302,  0.1755,  ...,  0.2680, -0.0229,  0.4409],\n",
      "         [ 0.0370,  0.1214,  0.5230,  ...,  0.3898,  0.1992,  0.1753],\n",
      "         [-0.0026,  0.2461,  0.2052,  ...,  0.1002,  0.5201,  0.4357]],\n",
      "\n",
      "        [[-0.1707, -0.2158, -0.5382,  ..., -0.2165, -0.4427, -0.6166],\n",
      "         [-0.8529, -0.7117, -0.2956,  ..., -0.5081, -0.2280, -0.6186],\n",
      "         [-0.3536, -0.5622, -0.2257,  ..., -0.5816, -0.4873, -0.0882],\n",
      "         ...,\n",
      "         [-0.3236, -0.5790, -0.1929,  ..., -0.4649, -0.5043, -0.6842],\n",
      "         [-0.3003, -0.4034, -0.4859,  ..., -0.5080, -0.5723, -0.3810],\n",
      "         [-0.5071, -0.6743, -0.6852,  ..., -0.5152, -0.1174, -0.6006]],\n",
      "\n",
      "        [[ 0.3080,  0.7471,  0.1995,  ...,  0.2001,  0.5979,  0.3039],\n",
      "         [ 0.4852,  0.5978,  0.4319,  ...,  0.4833,  0.1399,  0.5260],\n",
      "         [ 0.4826,  0.4843,  0.5471,  ...,  0.3068,  0.2997,  0.3518],\n",
      "         ...,\n",
      "         [ 0.1336,  0.3448,  0.2716,  ...,  0.5353,  0.3587,  0.2544],\n",
      "         [ 0.1752,  0.5470,  0.3395,  ...,  0.2586,  0.2970,  0.5606],\n",
      "         [ 0.0993,  0.5733,  0.7139,  ...,  0.8857,  0.3025,  0.1215]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3378,  0.8123,  0.6327,  ...,  0.5158,  0.5590,  0.4201],\n",
      "         [ 0.4262,  0.5598,  0.7421,  ...,  0.6060,  0.5343,  0.4358],\n",
      "         [ 0.6568,  0.3600,  0.5380,  ...,  1.1967,  0.9071,  0.8156],\n",
      "         ...,\n",
      "         [ 0.3227,  0.7769,  0.0418,  ...,  0.2247,  0.9435,  0.5999],\n",
      "         [ 0.4431,  0.6709,  0.8159,  ...,  0.7661,  0.5470,  0.4448],\n",
      "         [ 0.8717,  0.7225,  0.5593,  ...,  0.7157,  0.7661,  0.5997]],\n",
      "\n",
      "        [[-0.8892, -0.4422, -0.5848,  ..., -0.7318, -0.2747, -0.5929],\n",
      "         [-0.2594, -0.5827, -0.2503,  ..., -0.2535, -0.1947, -0.1857],\n",
      "         [-0.8834, -0.1026, -0.5914,  ...,  0.0406, -0.4827, -0.0935],\n",
      "         ...,\n",
      "         [-0.3792, -0.3211, -0.4488,  ...,  0.0448, -0.3395, -0.1040],\n",
      "         [-0.1986, -0.8214, -0.5787,  ..., -0.3729, -0.1375, -0.3797],\n",
      "         [-0.7646, -0.3454,  0.0702,  ..., -0.6666, -0.4603, -0.1234]],\n",
      "\n",
      "        [[-0.7924, -0.9549, -0.6127,  ..., -0.4825, -0.7709, -0.7548],\n",
      "         [-0.4939, -0.5083, -0.9074,  ..., -0.8261, -0.2936, -0.4845],\n",
      "         [-0.7622, -0.9882, -0.6497,  ..., -0.8379, -0.6117, -0.5560],\n",
      "         ...,\n",
      "         [-0.4593, -0.4404, -0.5228,  ..., -0.5077, -0.7412, -0.8846],\n",
      "         [-0.9737, -0.7799, -0.9781,  ..., -0.2598, -1.0383, -0.6741],\n",
      "         [-0.3467, -0.5346, -0.9050,  ..., -0.8086, -0.3045, -0.4631]]],\n",
      "       device='cuda:0')\n",
      "fc1\n",
      "tensor([[ 0.1345,  0.2501,  0.2290, -0.2285, -0.0177,  0.0487,  0.0811, -0.1176,\n",
      "         -0.0816,  0.3143,  0.0957,  0.2188, -0.0573,  0.2860,  0.3580,  0.0791,\n",
      "         -0.3179, -0.1054,  0.2217,  0.0480, -0.0359, -0.4283,  0.0335, -0.0580,\n",
      "          0.2904, -0.1352,  0.4798, -0.0968, -0.0909, -0.1975,  0.1527, -0.2937,\n",
      "         -0.0866, -0.1452,  0.0874, -0.1494, -0.1217,  0.0489, -0.0539,  0.0355,\n",
      "          0.2653, -0.2870, -0.2141,  0.5191,  0.1110, -0.0057,  0.2451, -0.1077,\n",
      "         -0.2184, -0.2353]], device='cuda:0')\n",
      "fc2\n",
      "tensor([[ 0.2093, -0.0684,  0.0239,  0.1148,  0.0492, -0.1170,  0.1870, -0.1152,\n",
      "         -0.0527,  0.1012]], device='cuda:0')\n",
      "<generator object Module.named_modules at 0x7f5a93efa820>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "hooks = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        hooks[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n",
    "    hooks[name] = module.register_forward_hook(get_activation(name))\n",
    "    output = model(x1)\n",
    "    print(hooks[name])\n",
    "lists = model.named_modules()\n",
    "print(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# the VGG11 architecture\n",
    "class VGG11(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(VGG11, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        # convolutional layers \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # fully connected linear layers\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Linear(in_features=4096, out_features=self.num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # flatten to prepare for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG11(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear_layers): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout2d(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout2d(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG11()\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8e91a9fb4a18180c8edf2062f1258c709cf2408f146570f2930cfcc1d509a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
