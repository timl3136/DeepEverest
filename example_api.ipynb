{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from DeepEverest import DeepEverest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])), batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, \n",
    "                        transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])), batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(320, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_159073/2494629665.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = torch.tensor(data)\n",
      "/tmp/ipykernel_159073/2494629665.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset_labels = torch.tensor(data_set)\n"
     ]
    }
   ],
   "source": [
    "dataset = torch.tensor([])\n",
    "dataset_labels = torch.tensor([])\n",
    "for num, (data, data_set) in enumerate(test_loader):\n",
    "    if num == 0:\n",
    "        dataset = torch.tensor(data)\n",
    "        dataset_labels = torch.tensor(data_set)\n",
    "    else:\n",
    "        dataset = torch.cat((dataset, data), 0)\n",
    "        dataset_labels = torch.cat((dataset_labels, data_set), 0)\n",
    "model = Net()\n",
    "lib_file = \"index/build/lib.linux-x86_64-3.8/deepeverst_index.cpython-38-x86_64-linux-gnu.so\"\n",
    "model = DeepEverest(model, True, lib_file, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct a new pytorch network, and construct the network into the DeepEverest API class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model = DeepEverest(model, True, lib_file, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a pretrained weight as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print out all the layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': <torch.utils.hooks.RemovableHandle at 0x7f70d1a42a90>,\n",
       " 'model': <torch.utils.hooks.RemovableHandle at 0x7f70d1a42790>,\n",
       " 'model.0': <torch.utils.hooks.RemovableHandle at 0x7f70d1a42e50>,\n",
       " 'model.1': <torch.utils.hooks.RemovableHandle at 0x7f70d1a42af0>,\n",
       " 'model.2': <torch.utils.hooks.RemovableHandle at 0x7f70d1a42be0>,\n",
       " 'model.3': <torch.utils.hooks.RemovableHandle at 0x7f70d1a42700>,\n",
       " 'model.4': <torch.utils.hooks.RemovableHandle at 0x7f70d1a42c10>,\n",
       " 'model.5': <torch.utils.hooks.RemovableHandle at 0x7f70d1a42c40>,\n",
       " 'model.6': <torch.utils.hooks.RemovableHandle at 0x7f720de74b80>,\n",
       " 'model.7': <torch.utils.hooks.RemovableHandle at 0x7f720de74610>,\n",
       " 'model.8': <torch.utils.hooks.RemovableHandle at 0x7f720de74b20>,\n",
       " 'model.9': <torch.utils.hooks.RemovableHandle at 0x7f720de74df0>,\n",
       " 'model.10': <torch.utils.hooks.RemovableHandle at 0x7f720de74850>,\n",
       " 'model.11': <torch.utils.hooks.RemovableHandle at 0x7f720de74910>,\n",
       " 'model.12': <torch.utils.hooks.RemovableHandle at 0x7f720de74a60>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 1,  ..., 4, 5, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.0\n",
      "model.1\n",
      "model.2\n",
      "model.3\n",
      "model.4\n",
      "model.5\n",
      "model.6\n",
      "model.7\n",
      "model.8\n",
      "model.9\n",
      "model.10\n",
      "model.11\n",
      "model.12\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.get_model().named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose one of the layer names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'model', 'model.0', 'model.1', 'model.2', 'model.3', 'model.4', 'model.5', 'model.6', 'model.7', 'model.8', 'model.9', 'model.10', 'model.11', 'model.12']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "all_layer_names = model.get_all_layer_names()\n",
    "print(all_layer_names)\n",
    "layer_name = \"model.2\"\n",
    "layer_id = all_layer_names.index(layer_name)\n",
    "print(layer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_result = model.get_layer_results_by_layer_id(layer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10, 12, 12)\n"
     ]
    }
   ],
   "source": [
    "print(layer_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the index before doing the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.construct_index(layer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give top-k activation neurons of image 659 as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(15.056664, (4, 4, 6)), (14.274166, (4, 5, 6)), (13.664486, (4, 6, 6)), (13.554054, (4, 3, 6)), (13.502932, (4, 8, 6)), (13.477571, (4, 9, 6)), (13.340709, (1, 11, 6)), (13.27995, (4, 4, 7)), (13.197538, (1, 5, 3)), (13.188957, (4, 3, 7)), (13.090613, (1, 10, 6)), (12.774042, (1, 5, 4)), (12.733601, (4, 2, 6)), (12.522223, (4, 7, 6)), (12.089676, (4, 5, 7)), (11.97962, (1, 11, 7)), (11.603155, (4, 8, 5)), (11.496715, (4, 2, 7)), (11.474282, (0, 3, 7)), (11.457584, (1, 4, 4))]]\n"
     ]
    }
   ],
   "source": [
    "topk_activations = model.get_topk_activations_given_images([659], layer_name, 20)\n",
    "print(topk_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 4, 6), (4, 5, 6), (4, 6, 6), (4, 3, 6), (4, 8, 6), (4, 9, 6), (1, 11, 6), (4, 4, 7), (1, 5, 3), (4, 3, 7), (1, 10, 6), (1, 5, 4), (4, 2, 6), (4, 7, 6), (4, 5, 7), (1, 11, 7), (4, 8, 5), (4, 2, 7), (0, 3, 7), (1, 4, 4)]\n"
     ]
    }
   ],
   "source": [
    "from NeuronGroup import *\n",
    "image_sample_id = 659\n",
    "topk_activations_neurons = [x[1] for x in topk_activations[0]]\n",
    "print(topk_activations_neurons)\n",
    "neuron_group = NeuronGroup(model.get_model(), layer_id, neuron_idx_list=topk_activations_neurons[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query the top 20 closest images with respect to the top 3 neurons found in the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 659, size of neuron group 3\n",
      "threshold: 0.10146905481815338, max in answer: 1.40303635597229, images run: 67\n",
      "threshold: 0.20118562877178192, max in answer: 1.1438370943069458, images run: 132\n",
      "threshold: 0.2788737714290619, max in answer: 0.9198803901672363, images run: 196\n",
      "threshold: 0.42958271503448486, max in answer: 0.7807649374008179, images run: 260\n",
      "threshold: 0.5854843854904175, max in answer: 0.7519282102584839, images run: 325\n",
      "threshold: 0.7778107523918152, max in answer: 0.7440550923347473, images run: 389\n",
      "======================= NTA exited =======================\n"
     ]
    }
   ],
   "source": [
    "top_k, exit_msg = model.answer_query_with_guarantee(image_sample_id, 20, neuron_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out distance and corresponding image indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.7440551, 2915), (-0.7409354, 6074), (-0.6970588, 8109), (-0.6478751, 7822), (-0.6055464, 3611), (-0.58405477, 6118), (-0.5718313, 4011), (-0.53731173, 8100), (-0.52412087, 4097), (-0.5215469, 4861), (-0.46935377, 6988), (-0.44762346, 6096), (-0.4275496, 8048), (-0.4268946, 7783), (-0.42228025, 1909), (-0.41333342, 3229), (-0.3712188, 1480), (-0.23766346, 8679), (-0.113409005, 8112), (0.0, 659)]\n",
      "termination: images run: 389\n"
     ]
    }
   ],
   "source": [
    "print(top_k)\n",
    "print(exit_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 321, 810]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids = [1, 0, 321, 810]\n",
    "image_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This tool is capable of search group of images in one query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_activations = model.get_topk_activations_given_images(image_ids, layer_name, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(15.404506, (1, 10, 6)),\n",
       "  (14.740844, (1, 10, 9)),\n",
       "  (14.725686, (1, 10, 5)),\n",
       "  (14.658377, (1, 10, 4)),\n",
       "  (14.074226, (1, 10, 7)),\n",
       "  (14.018891, (1, 10, 10)),\n",
       "  (13.745972, (1, 10, 3)),\n",
       "  (13.607746, (1, 2, 5)),\n",
       "  (13.291188, (1, 10, 8)),\n",
       "  (12.143644, (1, 2, 4)),\n",
       "  (11.776539, (4, 2, 6)),\n",
       "  (11.771938, (4, 6, 4)),\n",
       "  (11.722886, (4, 8, 3)),\n",
       "  (11.716561, (1, 1, 5)),\n",
       "  (11.585614, (4, 3, 6)),\n",
       "  (11.531162, (4, 7, 3)),\n",
       "  (11.222839, (4, 5, 4)),\n",
       "  (11.1286335, (4, 4, 5)),\n",
       "  (11.101066, (1, 10, 11)),\n",
       "  (11.002071, (4, 5, 5))],\n",
       " [(14.003622, (1, 4, 4)),\n",
       "  (13.753479, (1, 4, 5)),\n",
       "  (13.233683, (1, 4, 3)),\n",
       "  (13.210785, (1, 4, 6)),\n",
       "  (12.756104, (1, 4, 7)),\n",
       "  (10.772835, (4, 11, 4)),\n",
       "  (10.763884, (4, 10, 5)),\n",
       "  (10.652996, (4, 4, 8)),\n",
       "  (10.51455, (1, 3, 3)),\n",
       "  (10.362712, (1, 4, 2)),\n",
       "  (10.331228, (1, 3, 2)),\n",
       "  (9.836474, (4, 9, 5)),\n",
       "  (9.813205, (4, 5, 8)),\n",
       "  (9.546487, (4, 10, 4)),\n",
       "  (9.501915, (4, 8, 6)),\n",
       "  (9.265276, (4, 11, 5)),\n",
       "  (9.024993, (4, 7, 7)),\n",
       "  (8.871638, (4, 6, 7)),\n",
       "  (8.716935, (0, 4, 8)),\n",
       "  (8.675108, (4, 9, 6))],\n",
       " [(14.664563, (1, 3, 6)),\n",
       "  (12.218243, (1, 3, 4)),\n",
       "  (12.066067, (1, 3, 5)),\n",
       "  (11.853034, (1, 3, 7)),\n",
       "  (10.963835, (4, 3, 7)),\n",
       "  (10.242216, (1, 11, 4)),\n",
       "  (10.15063, (4, 6, 6)),\n",
       "  (10.149592, (4, 8, 5)),\n",
       "  (10.02931, (1, 11, 5)),\n",
       "  (9.914029, (4, 5, 6)),\n",
       "  (9.884069, (4, 4, 7)),\n",
       "  (9.840136, (4, 7, 5)),\n",
       "  (9.71972, (4, 10, 4)),\n",
       "  (9.673868, (4, 9, 4)),\n",
       "  (9.4589, (0, 3, 7)),\n",
       "  (9.257872, (4, 8, 4)),\n",
       "  (9.237775, (1, 4, 3)),\n",
       "  (9.144998, (4, 5, 7)),\n",
       "  (8.869293, (1, 4, 4)),\n",
       "  (8.831269, (0, 10, 5))],\n",
       " [(15.82171, (1, 4, 5)),\n",
       "  (15.716701, (1, 4, 4)),\n",
       "  (15.0590925, (1, 9, 9)),\n",
       "  (14.814839, (1, 9, 4)),\n",
       "  (14.55113, (1, 9, 8)),\n",
       "  (14.302644, (1, 4, 3)),\n",
       "  (14.064659, (4, 7, 7)),\n",
       "  (14.00155, (1, 9, 3)),\n",
       "  (13.664331, (1, 4, 6)),\n",
       "  (13.379918, (1, 3, 7)),\n",
       "  (13.368186, (4, 10, 5)),\n",
       "  (13.346981, (4, 4, 8)),\n",
       "  (13.32247, (4, 7, 6)),\n",
       "  (13.311966, (4, 6, 7)),\n",
       "  (13.224638, (4, 8, 7)),\n",
       "  (13.212799, (4, 8, 6)),\n",
       "  (13.198774, (4, 9, 5)),\n",
       "  (12.580459, (4, 9, 6)),\n",
       "  (12.509506, (4, 5, 7)),\n",
       "  (12.455315, (4, 8, 5))]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A group of neurons can be specified by the user to do the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sample_id = 810\n",
    "neuron_list = [(1, 4, 4), (1, 4, 3), (1, 4, 5)]\n",
    "neuron_group = NeuronGroup(model.get_model(), layer_id, neuron_idx_list=neuron_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the closest image with respect o the neuron group specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 810, size of neuron group 3\n",
      "threshold: 0.4012274742126465, max in answer: 1.244203805923462, images run: 67\n",
      "threshold: 0.7033581137657166, max in answer: 1.01547372341156, images run: 131\n",
      "threshold: 0.9670814871788025, max in answer: 0.9958177804946899, images run: 195\n",
      "threshold: 1.2530744075775146, max in answer: 0.9958177804946899, images run: 260\n",
      "======================= NTA exited =======================\n",
      "[(-0.9958178, 9990), (-0.9594661, 8555), (-0.9563361, 6316), (-0.92077124, 6050), (-0.88487566, 6158), (-0.86679006, 5179), (-0.85618293, 3743), (-0.84050524, 60), (-0.83489597, 1935), (-0.81305367, 7960), (-0.78954655, 2187), (-0.73703915, 6366), (-0.6998202, 5326), (-0.6811, 4865), (-0.6809903, 6006), (-0.65254635, 4064), (-0.6416337, 7650), (-0.6308938, 3125), (-0.57279134, 822), (0.0, 810)]\n"
     ]
    }
   ],
   "source": [
    "top_k, exit_msg = model.answer_query_with_guarantee(image_sample_id, 20, neuron_group)\n",
    "print(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_group = NeuronGroup(model.get_model(), layer_id, neuron_idx_list=[(5, 0, 2), (8, 2, 2), (2, 4, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 810, size of neuron group 3\n",
      "threshold: 0.03818821907043457, max in answer: 0.1384831666946411, images run: 427\n",
      "threshold: 0.20468735694885254, max in answer: 0.07035708427429199, images run: 851\n",
      "======================= NTA exited =======================\n",
      "[(-0.070357084, 4176), (-0.0684191, 4659), (-0.06708133, 3394), (-0.06519842, 9109), (-0.05986345, 2700), (-0.057710648, 554), (-0.052951217, 4042), (-0.045683503, 1100), (-0.04501462, 6053), (-0.04026103, 9618), (-0.039102912, 4421), (-0.03840089, 738), (-0.03818822, 812), (-0.03238964, 3431), (-0.026446223, 362), (-0.025786996, 924), (-0.024932742, 6871), (-0.020506978, 6075), (-0.0010017157, 2115), (0.0, 810)]\n"
     ]
    }
   ],
   "source": [
    "top_k, exit_msg = model.answer_query_with_guarantee(image_sample_id, 20, neuron_group)\n",
    "print(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(model.get_current_index_layer_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can construct indices for multiple layers, and the index constructed so far can be saved to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_index_map(\"saved_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 810, size of neuron group 3\n",
      "threshold: 0.03818821907043457, max in answer: 0.1384831666946411, images run: 427\n",
      "threshold: 0.20468735694885254, max in answer: 0.07035708427429199, images run: 851\n",
      "======================= NTA exited =======================\n",
      "[(-0.070357084, 4176), (-0.0684191, 4659), (-0.06708133, 3394), (-0.06519842, 9109), (-0.05986345, 2700), (-0.057710648, 554), (-0.052951217, 4042), (-0.045683503, 1100), (-0.04501462, 6053), (-0.04026103, 9618), (-0.039102912, 4421), (-0.03840089, 738), (-0.03818822, 812), (-0.03238964, 3431), (-0.026446223, 362), (-0.025786996, 924), (-0.024932742, 6871), (-0.020506978, 6075), (-0.0010017157, 2115), (0.0, 810)]\n"
     ]
    }
   ],
   "source": [
    "top_k, exit_msg = model.answer_query_with_guarantee(image_sample_id, 20, neuron_group)\n",
    "print(top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved indices can be reload back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_index_map(\"saved_map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This part is aimed to show that the code also works with Tensorflow model, the process is similar, and user should create a super class shown in MnistVGG_api class, and use this class to do queries. The procedure is very similar comparing to using a Pytorch model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from utils import load_mnist_vgg_dataset_model\n",
    "from models.MnistVGG_api import MnistVGG\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-21 01:46:56.988340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-21 01:46:56.996872: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-21 01:46:56.996896: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-21 01:46:56.997495: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_1', 'zero_padding2d', 'conv2d', 'batch_normalization', 'activation', 'conv2d_1', 'batch_normalization_1', 'activation_1', 'max_pooling2d', 'conv2d_2', 'batch_normalization_2', 'activation_2', 'conv2d_3', 'batch_normalization_3', 'activation_3', 'max_pooling2d_1', 'conv2d_4', 'batch_normalization_4', 'activation_4', 'conv2d_5', 'batch_normalization_5', 'activation_5', 'conv2d_6', 'batch_normalization_6', 'activation_6', 'max_pooling2d_2', 'conv2d_7', 'batch_normalization_7', 'activation_7', 'conv2d_8', 'batch_normalization_8', 'activation_8', 'conv2d_9', 'batch_normalization_9', 'activation_9', 'max_pooling2d_3', 'conv2d_10', 'batch_normalization_10', 'activation_10', 'conv2d_11', 'batch_normalization_11', 'activation_11', 'conv2d_12', 'batch_normalization_12', 'activation_12', 'max_pooling2d_4', 'flatten', 'dense', 'dropout', 'dense_1', 'dropout_1', 'dense_2', 'activation_13']\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_mnist_vgg_dataset_model()\n",
    "dataset = x_test\n",
    "dataset_labels = y_test\n",
    "del x_train\n",
    "del y_train\n",
    "model = MnistVGG(lib_file, dataset)\n",
    "all_layer_names = model.get_all_layer_names()\n",
    "print(all_layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = \"activation_12\"\n",
    "layer_id = all_layer_names.index(layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(dataset_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2, 2, 512)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "layer_result = model.get_layer_results_by_layer_id(layer_id)\n",
    "print(layer_result.shape)\n",
    "print(type(layer_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.construct_index(layer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(2.2470708, (1, 1, 374)), (1.9390898, (0, 0, 113)), (1.9011347, (0, 0, 358)), (1.8457984, (1, 0, 335)), (1.8241947, (1, 1, 62)), (1.7726848, (1, 0, 182)), (1.7536622, (1, 1, 129)), (1.7496964, (0, 0, 95)), (1.7491626, (1, 0, 441)), (1.6753774, (1, 1, 177)), (1.6667684, (0, 0, 46)), (1.6587316, (0, 1, 340)), (1.6440474, (0, 1, 285)), (1.6398696, (1, 1, 428)), (1.6205437, (0, 1, 365)), (1.6141273, (0, 1, 461)), (1.5758135, (0, 0, 245)), (1.558946, (0, 0, 45)), (1.5521429, (0, 1, 336)), (1.5128994, (0, 1, 379))]]\n"
     ]
    }
   ],
   "source": [
    "topk_activations = model.get_topk_activations_given_images([659], layer_name, 20)\n",
    "print(topk_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1, 374), (0, 0, 113), (0, 0, 358), (1, 0, 335), (1, 1, 62), (1, 0, 182), (1, 1, 129), (0, 0, 95), (1, 0, 441), (1, 1, 177), (0, 0, 46), (0, 1, 340), (0, 1, 285), (1, 1, 428), (0, 1, 365), (0, 1, 461), (0, 0, 245), (0, 0, 45), (0, 1, 336), (0, 1, 379)]\n"
     ]
    }
   ],
   "source": [
    "from NeuronGroup import *\n",
    "image_sample_id = 659\n",
    "topk_activations_neurons = [x[1] for x in topk_activations[0]]\n",
    "print(topk_activations_neurons)\n",
    "neuron_group = NeuronGroup(model.get_model(), layer_id, neuron_idx_list=topk_activations_neurons[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 659, size of neuron group 3\n",
      "threshold: 0.009084991179406643, max in answer: 1.0652904510498047, images run: 446\n",
      "threshold: 0.2361185997724533, max in answer: 0.7390391230583191, images run: 879\n",
      "threshold: 0.25425785779953003, max in answer: 0.7369704842567444, images run: 1320\n",
      "threshold: 0.4669073820114136, max in answer: 0.648984968662262, images run: 1707\n",
      "threshold: 0.5761292576789856, max in answer: 0.648984968662262, images run: 2125\n",
      "threshold: 0.7417832016944885, max in answer: 0.648984968662262, images run: 2537\n",
      "======================= NTA exited =======================\n"
     ]
    }
   ],
   "source": [
    "top_k, exit_msg = model.answer_query_with_guarantee(image_sample_id, 20, neuron_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.64898497, 8043), (-0.6488453, 3073), (-0.63240474, 2915), (-0.6309901, 9505), (-0.6308845, 7985), (-0.61490285, 9664), (-0.57323945, 1941), (-0.57080907, 9637), (-0.5610424, 1903), (-0.5584375, 3166), (-0.5519714, 468), (-0.536307, 5649), (-0.5042956, 667), (-0.3780876, 7902), (-0.37677968, 3055), (-0.37317902, 5655), (-0.3371267, 4176), (-0.32671073, 4199), (-0.15818696, 3808), (0.0, 659)]\n",
      "termination: images run: 2537\n"
     ]
    }
   ],
   "source": [
    "print(top_k)\n",
    "print(exit_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "print(model.get_current_index_layer_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_index_map(\"saved_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_index_map(\"saved_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 659, size of neuron group 3\n",
      "threshold: 0.009084991179406643, max in answer: 1.0652904510498047, images run: 446\n",
      "threshold: 0.2361185997724533, max in answer: 0.7390391230583191, images run: 879\n",
      "threshold: 0.25425785779953003, max in answer: 0.7369704842567444, images run: 1320\n",
      "threshold: 0.4669073820114136, max in answer: 0.648984968662262, images run: 1707\n",
      "threshold: 0.5761292576789856, max in answer: 0.648984968662262, images run: 2125\n",
      "threshold: 0.7417832016944885, max in answer: 0.648984968662262, images run: 2537\n",
      "======================= NTA exited =======================\n"
     ]
    }
   ],
   "source": [
    "top_k, exit_msg = model.answer_query_with_guarantee(image_sample_id, 20, neuron_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.64898497, 8043), (-0.6488453, 3073), (-0.63240474, 2915), (-0.6309901, 9505), (-0.6308845, 7985), (-0.61490285, 9664), (-0.57323945, 1941), (-0.57080907, 9637), (-0.5610424, 1903), (-0.5584375, 3166), (-0.5519714, 468), (-0.536307, 5649), (-0.5042956, 667), (-0.3780876, 7902), (-0.37677968, 3055), (-0.37317902, 5655), (-0.3371267, 4176), (-0.32671073, 4199), (-0.15818696, 3808), (0.0, 659)]\n",
      "termination: images run: 2537\n"
     ]
    }
   ],
   "source": [
    "print(top_k)\n",
    "print(exit_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = \"activation_10\"\n",
    "layer_id = all_layer_names.index(layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2, 2, 512)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "layer_result = model.get_layer_results_by_layer_id(layer_id)\n",
    "print(layer_result.shape)\n",
    "print(type(layer_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.construct_index(layer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_current_index_layer_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.construct_index(44)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
